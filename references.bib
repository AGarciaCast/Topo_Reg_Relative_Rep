
@misc{dosovitskiy_vit_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Just4Ref, Low, Unread},
}

@misc{fu_cyclical_2019,
	title = {Cyclical {Annealing} {Schedule}: {A} {Simple} {Approach} to {Mitigating} {KL} {Vanishing}},
	shorttitle = {Cyclical {Annealing} {Schedule}},
	url = {http://arxiv.org/abs/1903.10145},
	doi = {10.48550/arXiv.1903.10145},
	abstract = {Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter {\textbackslash}beta. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for {\textbackslash}beta, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing {\textbackslash}beta multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Fu, Hao and Li, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
	month = jun,
	year = {2019},
	note = {arXiv:1903.10145 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Unread},
}

@misc{noauthor_pytorch_2023,
	title = {{PyTorch} {Image} {Models}},
	copyright = {Apache-2.0},
	url = {https://github.com/huggingface/pytorch-image-models},
	abstract = {PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, EfficientNetV2, NFNet, Vision Transformer, MixNet, MobileNet-V3/V2, RegNet, DPN, CSPNet, and more},
	urldate = {2023-06-04},
	publisher = {Hugging Face},
	month = jun,
	year = {2023},
	note = {original-date: 2019-02-02T05:51:12Z},
	keywords = {Unread},
}

@misc{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	doi = {10.48550/arXiv.1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv:1312.5602 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning, Unread},
}

@misc{chang_advanced_2021,
	title = {Advanced {Techniques} for {Fine}-tuning {Transformers}},
	url = {https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e},
	abstract = {Learn these advanced techniques and see how they can help improve results},
	language = {en},
	urldate = {2023-06-03},
	journal = {Medium},
	author = {Chang, Peggy},
	month = nov,
	year = {2021},
	keywords = {Unread},
}

@misc{godfrey_symmetries_2023,
	title = {On the {Symmetries} of {Deep} {Learning} {Models} and their {Internal} {Representations}},
	url = {http://arxiv.org/abs/2205.14258},
	doi = {10.48550/arXiv.2205.14258},
	abstract = {Symmetry is a fundamental tool in the exploration of a broad range of complex systems. In machine learning symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family's internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. We connect intertwiner groups to a model's internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network's representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of concentrating model interpretability exploration on the activation basis in hidden layers rather than arbitrary linear combinations thereof.},
	urldate = {2023-04-02},
	publisher = {arXiv},
	author = {Godfrey, Charles and Brown, Davis and Emerson, Tegan and Kvinge, Henry},
	month = mar,
	year = {2023},
	note = {arXiv:2205.14258 [cs]},
	keywords = {68T07 (Primary) 20C35, 62H20 (Secondary), Computer Science - Artificial Intelligence, Computer Science - Machine Learning, G.3, I.2, Unread},
}

@misc{cannistraci_bootstrapping_2023,
	title = {Bootstrapping {Parallel} {Anchors} for {Relative} {Representations}},
	url = {http://arxiv.org/abs/2303.00721},
	abstract = {The use of relative representations for latent embeddings has shown potential in enabling latent space communication and zero-shot model stitching across a wide range of applications. Nevertheless, relative representations rely on a certain amount of parallel anchors to be given as input, which can be impractical to obtain in certain scenarios. To overcome this limitation, we propose an optimization-based method to discover new parallel anchors from a limited number of seeds. Our approach can be used to find semantic correspondence between different domains, align their relative spaces, and achieve competitive results in several tasks.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Cannistraci, Irene and Moschella, Luca and Maiorca, Valentino and Fumero, Marco and Norelli, Antonio and Rodolà, Emanuele},
	month = mar,
	year = {2023},
	note = {arXiv:2303.00721 [cs]},
	keywords = {68T07, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.6, Main, Unread},
}

@inproceedings{keung_multilingual_2020,
	address = {Online},
	title = {The {Multilingual} {Amazon} {Reviews} {Corpus}},
	url = {https://aclanthology.org/2020.emnlp-main.369},
	doi = {10.18653/v1/2020.emnlp-main.369},
	abstract = {We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., `books', `appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20\% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.},
	urldate = {2023-06-03},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Keung, Phillip and Lu, Yichao and Szarvas, György and Smith, Noah A.},
	month = nov,
	year = {2020},
	keywords = {Just4Ref, Low, Unread},
	pages = {4563--4568},
}

@misc{roeder_linear_2020,
	title = {On {Linear} {Identifiability} of {Learned} {Representations}},
	url = {http://arxiv.org/abs/2007.00810},
	abstract = {Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions typically lack identifiability in parameter space, because they are overparameterized by design. In this paper, building on recent advances in nonlinear ICA, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Roeder, Geoffrey and Metz, Luke and Kingma, Diederik P.},
	month = jul,
	year = {2020},
	note = {arXiv:2007.00810 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Unread},
}

@article{chen_topological_2023,
	title = {Topological {Regularization} for {Representation} {Learning} via {Persistent} {Homology}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/11/4/1008},
	doi = {10.3390/math11041008},
	abstract = {Generalization is challenging in small-sample-size regimes with over-parameterized deep neural networks, and a better representation is generally beneficial for generalization. In this paper, we present a novel method for controlling the internal representation of deep neural networks from a topological perspective. Leveraging the power of topology data analysis (TDA), we study the push-forward probability measure induced by the feature extractor, and we formulate a notion of “separation” to characterize a property of this measure in terms of persistent homology for the first time. Moreover, we perform a theoretical analysis of this property and prove that enforcing this property leads to better generalization. To impose this property, we propose a novel weight function to extract topological information, and we introduce a new regularizer including three items to guide the representation learning in a topology-aware manner. Experimental results in the point cloud optimization task show that our method is effective and powerful. Furthermore, results in the image classification task show that our method outperforms the previous methods by a significant margin.},
	language = {en},
	number = {4},
	urldate = {2023-03-28},
	journal = {Mathematics},
	author = {Chen, Muyi and Wang, Daling and Feng, Shi and Zhang, Yifei},
	month = jan,
	year = {2023},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Unread, deep neural network, persistent homology, push-forward probability measure, representation space},
	pages = {1008},
}

@misc{mahaut_referential_2023,
	title = {Referential communication in heterogeneous communities of pre-trained visual deep networks},
	url = {http://arxiv.org/abs/2302.08913},
	abstract = {As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of referential communication in a community of state-of-the-art pre-trained visual networks, showing that they can develop a shared protocol to refer to a target image among a set of candidates. Such shared protocol, induced in a self-supervised way, can to some extent be used to communicate about previously unseen object categories, as well as to make more granular distinctions compared to the categories taught to the original networks. Contradicting a common view in multi-agent emergent communication research, we find that imposing a discrete bottleneck on communication hampers the emergence of a general code. Moreover, we show that a new neural network can learn the shared protocol developed in a community with remarkable ease, and the process of integrating a new agent into a community more stably succeeds when the original community includes a larger set of heterogeneous networks. Finally, we illustrate the independent benefits of developing a shared communication layer by using it to directly transfer an object classifier from a network to another, and we qualitatively and quantitatively study its emergent properties.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Mahaut, Matéo and Franzon, Francesca and Dessì, Roberto and Baroni, Marco},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08913 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, High, Unread},
}

@inproceedings{viola_rapid_2001,
	address = {Kauai, HI, USA},
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	isbn = {978-0-7695-1272-3},
	url = {http://ieeexplore.ieee.org/document/990517/},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The ﬁrst is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efﬁcient classiﬁers[6]. The third contribution is a method for combining increasingly more complex classiﬁers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object speciﬁc focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	language = {en},
	urldate = {2023-03-17},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	publisher = {IEEE Comput. Soc},
	author = {Viola, P. and Jones, M.},
	year = {2001},
	keywords = {Just4Ref, Unread},
	pages = {I--511--I--518},
}

@inproceedings{gretton_measuring_2005,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Measuring {Statistical} {Dependence} with {Hilbert}-{Schmidt} {Norms}},
	isbn = {978-3-540-31696-1},
	abstract = {We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently published ICA methods.},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer},
	author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Schölkopf, Bernhard},
	editor = {Jain, Sanjay and Simon, Hans Ulrich and Tomita, Etsuji},
	year = {2005},
	keywords = {Covariance Operator, Independence Criterion, Independent Component Analysis, Just4Ref, Reproduce Kernel Hilbert Space, Unread},
	pages = {63--77},
}

@article{wang_finding_2020,
	title = {Finding the needle in a high-dimensional haystack: {Canonical} correlation analysis for neuroscientists},
	volume = {216},
	issn = {1053-8119},
	shorttitle = {Finding the needle in a high-dimensional haystack},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811920302329},
	doi = {10.1016/j.neuroimage.2020.116745},
	abstract = {The 21st century marks the emergence of “big data” with a rapid increase in the availability of datasets with multiple measurements. In neuroscience, brain-imaging datasets are more commonly accompanied by dozens or hundreds of phenotypic subject descriptors on the behavioral, neural, and genomic level. The complexity of such “big data” repositories offer new opportunities and pose new challenges for systems neuroscience. Canonical correlation analysis (CCA) is a prototypical family of methods that is useful in identifying the links between variable sets from different modalities. Importantly, CCA is well suited to describing relationships across multiple sets of data, such as in recently available big biomedical datasets. Our primer discusses the rationale, promises, and pitfalls of CCA.},
	language = {en},
	urldate = {2023-03-17},
	journal = {NeuroImage},
	author = {Wang, Hao-Ting and Smallwood, Jonathan and Mourao-Miranda, Janaina and Xia, Cedric Huchuan and Satterthwaite, Theodore D. and Bassett, Danielle S. and Bzdok, Danilo},
	month = aug,
	year = {2020},
	keywords = {Big data, Data science, Deep phenotyping, Just4Ref, Machine learning, Modality fusion, Unread},
	pages = {116745},
}

@misc{mcneely-white_exploring_2021,
	title = {Exploring the {Interchangeability} of {CNN} {Embedding} {Spaces}},
	url = {http://arxiv.org/abs/2010.02323},
	abstract = {CNN feature spaces can be linearly mapped and consequently are often interchangeable. This equivalence holds across variations in architectures, training datasets, and network tasks. Specifically, we mapped between 10 image-classification CNNs and between 4 facial-recognition CNNs. When image embeddings generated by one CNN are transformed into embeddings corresponding to the feature space of a second CNN trained on the same task, their respective image classification or face verification performance is largely preserved. For CNNs trained to the same classes and sharing a common backend-logit (soft-max) architecture, a linear-mapping may always be calculated directly from the backend layer weights. However, the case of a closed-set analysis with perfect knowledge of classifiers is limiting. Therefore, empirical methods of estimating mappings are presented for both the closed-set image classification task and the open-set task of face recognition. The results presented expose the essentially interchangeable nature of CNNs embeddings for two important and common recognition tasks. The implications are far-reaching, suggesting an underlying commonality between representations learned by networks designed and trained for a common task. One practical implication is that face embeddings from some commonly used CNNs can be compared using these mappings.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {McNeely-White, David and Sattelberg, Benjamin and Blanchard, Nathaniel and Beveridge, Ross},
	month = feb,
	year = {2021},
	note = {arXiv:2010.02323 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Just4Ref, Low, Unread},
}

@misc{sucholutsky_alignment_2023,
	title = {Alignment with human representations supports robust few-shot learning},
	url = {http://arxiv.org/abs/2301.11990},
	abstract = {Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Sucholutsky, Ilia and Griffiths, Thomas L.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11990 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, High, Statistics - Machine Learning, Unread},
}

@book{gonzalez_digital_2002,
	title = {Digital {Image} {Processing}},
	isbn = {978-0-201-18075-6},
	abstract = {"Digital Image Processing" has been the leading textbook in its field for more than 20 years. As was the case with the 1977 and 1987 editions by Gonzalez and Wintz, and the 1992 edition by Gonzalez and Woods, the present edition was prepared with students and instructors in mind. 771e material is timely, highly readable, and illustrated with numerous examples of practical significance. All mainstream areas of image processing are covered, including a totally revised introduction and discussion of image fundamentals, image enhancement in the spatial and frequency domains, restoration, color image processing, wavelets, image compression, morphology, segmentation, and image description. Coverage concludes with a discussion of the fundamentals of object recognition.  Although the book is completely self-contained, a Companion Website (see inside front cover) provides additional support in the form of review material, answers to selected problems, laboratory project suggestions. and a score of other features. A supplementary instructor's manual is available to instructors who have adopted the book for classroom use. "New Features"  New chapters on wavelets, image morphology, and color image processing. More than 500 new images and over 200 new line drawings and tables. A revision and update of all chapters, including topics such as segmentation by watersheds. Numerous new examples with processed images of higher resolution. A reorganization that allows the reader to get to the material on actualimage processing much sooner than before. Updated image compression standards and a new section on compression using wavelets. A more intuitive development of traditional topics such as image transforms and image restoration. Updated bibliography.},
	language = {en},
	publisher = {Prentice Hall},
	author = {Gonzalez, Rafael C. and Woods, Richard Eugene},
	year = {2002},
	note = {Google-Books-ID: 738oAQAAMAAJ},
	keywords = {Unread},
}

@misc{cmdlinetips_introduction_2020,
	title = {Introduction to {Canonical} {Correlation} {Analysis} ({CCA}) in {R}},
	url = {https://cmdlinetips.com/2020/12/canonical-correlation-analysis-in-r/},
	abstract = {Canonical Correlation Analysis (CCA) in R},
	language = {en-US},
	urldate = {2023-03-17},
	journal = {Python and R Tips},
	author = {cmdlinetips},
	month = dec,
	year = {2020},
	keywords = {Unread},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	doi = {10.48550/arXiv.2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Just4Ref, Low, Unread},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\%absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	keywords = {Just4Ref, Medium, Unread},
	pages = {4171--4186},
}

@misc{guo_zero-shot_2022,
	title = {Zero-{Shot} and {Few}-{Shot} {Learning} for {Lung} {Cancer} {Multi}-{Label} {Classification} using {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2205.15290},
	doi = {10.48550/arXiv.2205.15290},
	abstract = {Lung cancer is the leading cause of cancer-related death worldwide. Lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most common histologic subtypes of non-small-cell lung cancer (NSCLC). Histology is an essential tool for lung cancer diagnosis. Pathologists make classifications according to the dominant subtypes. Although morphology remains the standard for diagnosis, significant tool needs to be developed to elucidate the diagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT) model to classify multiple label lung cancer on histologic slices (from dataset LC25000), in both Zero-Shot and Few-Shot settings. Then we compare the performance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall, sensitivity and specificity. Our study show that the pre-trained ViT model has a good performance in Zero-Shot setting, a competitive accuracy (99.87\%) in Few-Shot setting (epoch = 1) and an optimal result (100.00\% on both validation set and test set) in Few-Shot seeting (epoch = 5).},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Guo, Fu-Ming and Fan, Yingfang},
	month = may,
	year = {2022},
	note = {arXiv:2205.15290 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Low, Unread},
}

@incollection{torres_hopfield_2019,
	address = {New York, NY},
	title = {Hopfield {Network}},
	isbn = {978-1-4614-7320-6},
	language = {en},
	urldate = {2023-03-12},
	booktitle = {Encyclopedia of {Computational} {Neuroscience}},
	publisher = {Springer},
	author = {Torres, Joaquín J.},
	editor = {Jaeger, Dieter and Jung, Ranu},
	year = {2019},
	keywords = {Unread},
	pages = {1--3},
}

@inproceedings{schonenberger_witness_2022,
	title = {Witness {Autoencoder}: {Shaping} the {Latent} {Space} with {Witness} {Complexes}},
	shorttitle = {Witness {Autoencoder}},
	abstract = {We present a Witness Autoencoder (W-AE) – an autoencoder that captures geodesic distances of the data in the latent space. Our algorithm uses witness complexes to compute geodesic distance approximations on a mini-batch level, and leverages topological information from the entire dataset while performing batch-wise approximations. This way, our method allows to capture the global structure of the data even with a small batch size, which is beneficial for large-scale real-world data. We show that our method captures the structure of the manifold more accurately than the recently introduced topological autoencoder (TopoAE).},
	language = {en},
	urldate = {2023-01-25},
	booktitle = {Witness {Autoencoder}: {Shaping} the {Latent} {Space} with {Witness} {Complexes}},
	author = {Schönenberger, Simon Till and Varava, Anastasiia and Polianskii, Vladislav and Chung, Jen Jen and Kragic, Danica and Siegwart, Roland},
	month = jul,
	year = {2022},
	keywords = {High, Unread},
}

@inproceedings{cubuk_autoaugment_2019,
	address = {Long Beach, CA, USA},
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{AutoAugment}},
	url = {https://ieeexplore.ieee.org/document/8953317/},
	doi = {10.1109/CVPR.2019.00020},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	month = jun,
	year = {2019},
	keywords = {Just4Ref, Low, Unread},
	pages = {113--123},
}

@article{cohen-steiner_stability_2007,
	title = {Stability of {Persistence} {Diagrams}},
	volume = {37},
	issn = {0179-5376, 1432-0444},
	url = {http://link.springer.com/10.1007/s00454-006-1276-5},
	doi = {10.1007/s00454-006-1276-5},
	language = {en},
	number = {1},
	urldate = {2023-02-21},
	journal = {Discrete and Computational Geometry},
	author = {Cohen-Steiner, David and Edelsbrunner, Herbert and Harer, John},
	month = jan,
	year = {2007},
	keywords = {Unread},
	pages = {103--120},
}

@article{carriere_stable_2015,
	title = {Stable {Topological} {Signatures} for {Points} on {3D} {Shapes}},
	volume = {34},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12692},
	doi = {10.1111/cgf.12692},
	abstract = {Comparing points on 3D shapes is among the fundamental operations in shape analysis. To facilitate this task, a great number of local point signatures or descriptors have been proposed in the past decades. However, the vast majority of these descriptors concentrate on the local geometry of the shape around the point, and thus are insensitive to its connectivity structure. By contrast, several global signatures have been proposed that successfully capture the overall topology of the shape and thus characterize the shape as a whole. In this paper, we propose the first point descriptor that captures the topology structure of the shape as ‘seen’ from a single point, in a multiscale and provably stable way. We also demonstrate how a large class of topological signatures, including ours, can be mapped to vectors, opening the door to many classical analysis and learning methods. We illustrate the performance of this approach on the problems of supervised shape labeling and shape matching. We show that our signatures provide complementary information to existing ones and allow to achieve better performance with less training data in both applications.},
	language = {en},
	number = {5},
	urldate = {2023-02-25},
	journal = {Computer Graphics Forum},
	author = {Carrière, Mathieu and Oudot, Steve Y. and Ovsjanikov, Maks},
	year = {2015},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12692},
	keywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.5 Computer Graphics: Computational Geometry and Object Modeling—Geometric algorithms, Unread, and systems, languages},
	pages = {1--12},
}

@article{amezquita_shape_2020,
	title = {The shape of things to come: {Topological} data analysis and biology, from molecules to organisms},
	volume = {249},
	issn = {1097-0177},
	shorttitle = {The shape of things to come},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/dvdy.175},
	doi = {10.1002/dvdy.175},
	abstract = {Shape is data and data is shape. Biologists are accustomed to thinking about how the shape of biomolecules, cells, tissues, and organisms arise from the effects of genetics, development, and the environment. Less often do we consider that data itself has shape and structure, or that it is possible to measure the shape of data and analyze it. Here, we review applications of topological data analysis (TDA) to biology in a way accessible to biologists and applied mathematicians alike. TDA uses principles from algebraic topology to comprehensively measure shape in data sets. Using a function that relates the similarity of data points to each other, we can monitor the evolution of topological features—connected components, loops, and voids. This evolution, a topological signature, concisely summarizes large, complex data sets. We first provide a TDA primer for biologists before exploring the use of TDA across biological sub-disciplines, spanning structural biology, molecular biology, evolution, and development. We end by comparing and contrasting different TDA approaches and the potential for their use in biology. The vision of TDA, that data are shape and shape is data, will be relevant as biology transitions into a data-driven era where the meaningful interpretation of large data sets is a limiting factor.},
	language = {en},
	number = {7},
	urldate = {2023-02-25},
	journal = {Developmental Dynamics},
	author = {Amézquita, Erik J. and Quigley, Michelle Y. and Ophelders, Tim and Munch, Elizabeth and Chitwood, Daniel H.},
	year = {2020},
	note = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/dvdy.175},
	keywords = {Unread, biology, data science, mathematical biology, persistent homology, shape, topological data analysis},
	pages = {816--833},
}

@misc{boissonnat_efficient_2018,
	title = {An {Efficient} {Representation} for {Filtrations} of {Simplicial} {Complexes}},
	url = {http://arxiv.org/abs/1607.08449},
	doi = {10.48550/arXiv.1607.08449},
	abstract = {A filtration over a simplicial complex K is an ordering of the simplices of  such that all prefixes in the ordering are subcomplexes of K. Filtrations are at the core of Persistent Homology, a major tool in Topological Data Analysis. In order to represent the filtration of a simplicial complex, the entire filtration can be appended to any data structure that explicitly stores all the simplices of the complex such as the Hasse diagram or the recently introduced Simplex Tree [Algorithmica '14]. However, with the popularity of various computational methods that need to handle simplicial complexes, and with the rapidly increasing size of the complexes, the task of finding a compact data structure that can still support efficient queries is of great interest. In this paper, we propose a new data structure called the Critical Simplex Diagram (CSD) which is a variant of the Simplex Array List (SAL) [Algorithmica '17]. Our data structure allows one to store in a compact way the filtration of a simplicial complex, and allows for the efficient implementation of a large range of basic operations. Moreover, we prove that our data structure is essentially optimal with respect to the requisite storage space. Finally, we show that the CSD representation admits fast construction algorithms for Flag complexes and relaxed Delaunay complexes.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Boissonnat, Jean-Daniel and S., Karthik C.},
	month = feb,
	year = {2018},
	note = {arXiv:1607.08449 [cs]},
	keywords = {Just4Ref, Low, Unread},
}

@article{brejcha_perception-driven_2021,
	title = {Perception-driven dynamics of mimicry based on attractor field model},
	volume = {11},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsfs.2020.0052},
	doi = {10.1098/rsfs.2020.0052},
	abstract = {We provide a formal account of an interface that bridges two different levels of dynamic processes manifested by mimicry: prey–prey interactions and predators' perception. Mimicry is a coevolutionary process between an animate selective agent and at least two similar organisms selected by agent's perception-driven actions. Attractor field model explains perceived similarity of forms by noting that in both human and animal cognition, morphologically intermediate forms are more likely to be perceived as belonging to rare rather than abundant forms. We formalize this model in terms of predators' perception space deformation using numerical simulations and argue that the probability of confusion between similar species creates pressure on the perception space, which in turn leads to inflation of regions of perception space with high density of species representations. Such inflation causes increased discrimination between species by a predator, which implies that adaptive mimicry could initially emerge more easily among atypical species because they do not need the same level of similarity to the model. We provide a theoretical instrument to conceptualize interdependence between objective measurable matrices and perceived matrices of the same external reality. We believe that our framework leads to a more precise understanding of the evolution of mimicry.},
	number = {3},
	urldate = {2023-03-12},
	journal = {Interface Focus},
	author = {Brejcha, Jindřich and Tureček, Petr and Kleisner, Karel},
	month = apr,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {Batesian mimicry, Müllerian mimicry, Unread, agency, behaviour, frequency dependence, similarity},
	pages = {20200052},
}

@misc{raghu_svcca_2017,
	title = {{SVCCA}: {Singular} {Vector} {Canonical} {Correlation} {Analysis} for {Deep} {Learning} {Dynamics} and {Interpretability}},
	shorttitle = {{SVCCA}},
	url = {http://arxiv.org/abs/1706.05806},
	abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/},
	urldate = {2023-03-11},
	publisher = {arXiv},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	month = nov,
	year = {2017},
	note = {arXiv:1706.05806 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Unread},
}

@misc{rauker_toward_2023,
	title = {Toward {Transparent} {AI}: {A} {Survey} on {Interpreting} the {Inner} {Structures} of {Deep} {Neural} {Networks}},
	shorttitle = {Toward {Transparent} {AI}},
	url = {http://arxiv.org/abs/2207.13243},
	abstract = {The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Räuker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
	month = jan,
	year = {2023},
	note = {arXiv:2207.13243 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Unread},
}

@inproceedings{barannikov_representation_2022,
	title = {Representation {Topology} {Divergence}: {A} {Method} for {Comparing} {Neural} {Network} {Representations}.},
	shorttitle = {Representation {Topology} {Divergence}},
	url = {https://proceedings.mlr.press/v162/barannikov22a.html},
	abstract = {Comparison of data representations is a complex multi-aspect problem. We propose a method for comparing two data representations. We introduce the Representation Topology Divergence (RTD) score measuring the dissimilarity in multi-scale topology between two point clouds of equal size with a one-to-one correspondence between points. The two data point clouds can lie in different ambient spaces. The RTD score is one of the few topological data analysis based practical methods applicable to real machine learning datasets. Experiments show the agreement of RTD with the intuitive assessment of data representation similarity. The proposed RTD score is sensitive to the data representation’s fine topological structure. We use the RTD score to gain insights on neural networks representations in computer vision and NLP domains for various problems: training dynamics analysis, data distribution shift, transfer learning, ensemble learning, disentanglement assessment.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Barannikov, Serguei and Trofimov, Ilya and Balabin, Nikita and Burnaev, Evgeny},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {Just4Ref, Medium, Metric, Similarity, Unread},
	pages = {1607--1626},
}

@phdthesis{medbouhi_towards_2022,
	title = {Towards topology-aware {Variational} {Auto}-{Encoders} : from {InvMap}-{VAE} to {Witness} {Simplicial} {VAE}},
	shorttitle = {Towards topology-aware {Variational} {Auto}-{Encoders}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-309487},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-18},
	school = {KTH Royal Institute of Technology},
	author = {Medbouhi, Aniss Aiman},
	year = {2022},
	keywords = {Medium, Unread},
}

@misc{noauthor_gudhi_nodate,
	title = {{GUDHI}: {Witness} complex},
	url = {https://gudhi.inria.fr/doc/latest/group__witness__complex.html},
	urldate = {2023-03-07},
	keywords = {Unread},
}

@article{dowker_homology_1952,
	title = {Homology {Groups} of {Relations}},
	volume = {56},
	issn = {0003-486X},
	url = {https://www.jstor.org/stable/1969768},
	doi = {10.2307/1969768},
	number = {1},
	urldate = {2023-03-07},
	journal = {Annals of Mathematics},
	author = {Dowker, C. H.},
	year = {1952},
	note = {Publisher: Annals of Mathematics},
	keywords = {Unread},
	pages = {84--95},
}

@article{silva_topological_2004,
	title = {Topological estimation using witness complexes},
	issn = {1811-7813},
	url = {http://diglib.eg.org/handle/10.2312/SPBG.SPBG04.157-166},
	doi = {10.2312/SPBG/SPBG04/157-166},
	abstract = {This paper tackles the problem of computing topological invariants of geometric objects in a robust manner, using only point cloud data sampled from the object. It is now widely recognised that this kind of topological analysis can give qualitative information about data sets which is not readily available by other means. In particular, it can be an aid to visualisation of high dimensional data. Standard simplicial complexes for approximating the topological type of the underlying space (such as Cech, Rips, or a-shape) produce simplicial complexes whose vertex set has the same size as the underlying set of point cloud data. Such constructions are sometimes still tractable, but are wasteful (of computing resources) since the homotopy types of the underlying objects are generally realisable on much smaller vertex sets. We obtain smaller complexes by choosing a set of landmark points from our data set, and then constructing a "witness complex" on this set using ideas motivated by the usual Delaunay complex in Euclidean space. The key idea is that the remaining (non-landmark) data points are used as witnesses to the existence of edges or simplices spanned by combinations of landmark points. Our construction generalises the topology-preserving graphs of Martinetz and Schulten [MS94] in two directions. First, it produces a simplicial complex rather than a graph. Secondly it actually produces a nested family of simplicial complexes, which represent the data at different feature scales, suitable for calculating persistent homology [ELZ00, ZC04]. We find that in addition to the complexes being smaller, they also provide (in a precise sense) a better picture of the homology, with less noise, than the full scale constructions using all the data points. We illustrate the use of these complexes in qualitatively analyzing a data set of 3x3 pixel patches studied by David Mumford et al [LPM03].},
	language = {en},
	urldate = {2023-03-07},
	journal = {SPBG'04 Symposium on Point - Based Graphics 2004},
	author = {Silva, Vin De and Carlsson, Gunnar},
	year = {2004},
	note = {Artwork Size: 10 pages
ISBN: 9783905673098
Publisher: The Eurographics Association},
	keywords = {Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computing Methodologies]: Computer Graphics [Computational Geometry and Object Modeling], Just4Ref, Medium, Unread},
	pages = {10 pages},
}

@inproceedings{zheng_topological_2022,
	title = {Topological {Detection} of {Trojaned} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=1r2EannVuIA},
	abstract = {Deep neural networks are known to have security issues. One particular threat is the Trojan attack. It occurs when the attackers stealthily manipulate the model's behavior through Trojaned training samples, which can later be exploited. Guided by basic neuroscientific principles, we discover subtle -- yet critical -- structural deviation characterizing Trojaned models. In our analysis we use topological tools. They allow us to model high-order dependencies in the networks, robustly compare different networks, and localize structural abnormalities. One interesting observation is that Trojaned models develop short-cuts from shallow to deep layers. Inspired by these observations, we devise a strategy for robust detection of Trojaned models. Compared to standard baselines it displays better performance on multiple benchmarks.},
	language = {en},
	urldate = {2023-02-26},
	booktitle = {Topological {Detection} of {Trojaned} {Neural} {Networks}},
	author = {Zheng, Songzhu and Zhang, Yikai and Wagner, Hubert and Goswami, Mayank and Chen, Chao},
	month = jan,
	year = {2022},
	keywords = {Unread},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Unread},
}

@article{tenenbaum_global_2000,
	title = {A {Global} {Geometric} {Framework} for {Nonlinear} {Dimensionality} {Reduction}},
	volume = {290},
	url = {https://www.science.org/doi/10.1126/science.290.5500.2319},
	doi = {10.1126/science.290.5500.2319},
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 106 optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	number = {5500},
	urldate = {2023-02-26},
	journal = {Science},
	author = {Tenenbaum, Joshua B. and Silva, Vin de and Langford, John C.},
	month = dec,
	year = {2000},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {Unread},
	pages = {2319--2323},
}

@misc{adams_persistence_2016,
	title = {Persistence {Images}: {A} {Stable} {Vector} {Representation} of {Persistent} {Homology}},
	shorttitle = {Persistence {Images}},
	url = {http://arxiv.org/abs/1507.06217},
	doi = {10.48550/arXiv.1507.06217},
	abstract = {Many datasets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a dataset. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite-dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Adams, Henry and Chepushtanova, Sofya and Emerson, Tegan and Hanson, Eric and Kirby, Michael and Motta, Francis and Neville, Rachel and Peterson, Chris and Shipman, Patrick and Ziegelmeier, Lori},
	month = jul,
	year = {2016},
	note = {arXiv:1507.06217 [cs, math, stat]},
	keywords = {Computer Science - Computational Geometry, F.2.2, I.5.2, Mathematics - Algebraic Topology, Statistics - Machine Learning, Unread},
}

@misc{bubenik_statistical_2015,
	title = {Statistical topological data analysis using persistence landscapes},
	url = {http://arxiv.org/abs/1207.6437},
	doi = {10.48550/arXiv.1207.6437},
	abstract = {We define a new topological summary for data that we call the persistence landscape. Since this summary lies in a vector space, it is easy to combine with tools from statistics and machine learning, in contrast to the standard topological summaries. Viewed as a random variable with values in a Banach space, this summary obeys a strong law of large numbers and a central limit theorem. We show how a number of standard statistical tests can be used for statistical inference using this summary. We also prove that this summary is stable and that it can be used to provide lower bounds for the bottleneck and Wasserstein distances.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Bubenik, Peter},
	month = jan,
	year = {2015},
	note = {arXiv:1207.6437 [cs, math, stat]},
	keywords = {55N99, 68W30, 62G99, 54E35, Computer Science - Computational Geometry, Mathematics - Algebraic Topology, Mathematics - Metric Geometry, Mathematics - Statistics Theory, Unread},
}

@article{pirashvili_improved_2018,
	title = {Improved understanding of aqueous solubility modeling through {Topological} {Data} {Analysis}},
	volume = {10},
	doi = {10.1186/s13321-018-0308-5},
	abstract = {Topological data analysis is a family of recent mathematical techniques seeking to understand the ‘shape’ of data, and has been used to understand the structure of the descriptor space produced from a standard chemical informatics software from the point of view of solubility. We have used the mapper algorithm, a TDA method that creates low-dimensional representations of data, to create a network visualization of the solubility space. While descriptors with clear chemical implications are prominent features in this space, reflecting their importance to the chemical properties, an unexpected and interesting correlation between chlorine content and rings and their implication for solubility prediction is revealed. A parallel representation of the chemical space was generated using persistent homology applied to molecular graphs. Links between this chemical space and the descriptor space were shown to be in agreement with chemical heuristics. The use of persistent homology on molecular graphs, extended by the use of norms on the associated persistence landscapes allow the conversion of discrete shape descriptors to continuous ones, and a perspective of the application of these descriptors to quantitative structure property relations is presented.},
	journal = {Journal of Cheminformatics},
	author = {Pirashvili, Mariam and Steinberg, Lee and Belchí, Francisco and Niranjan, Mahesan and Frey, Jeremy and Brodzki, Jacek},
	month = nov,
	year = {2018},
	keywords = {Unread},
}

@article{agerberg_supervised_2021,
	title = {Supervised {Learning} {Using} {Homology} {Stable} {Rank} {Kernels}},
	volume = {7},
	issn = {2297-4687},
	url = {https://www.frontiersin.org/articles/10.3389/fams.2021.668046},
	abstract = {Exciting recent developments in Topological Data Analysis have aimed at combining homology-based invariants with Machine Learning. In this article, we use hierarchical stabilization to bridge between persistence and kernel-based methods by introducing the so-called stable rank kernels. A fundamental property of the stable rank kernels is that they depend on metrics to compare persistence modules. We illustrate their use on artificial and real-world datasets and show that by varying the metric we can improve accuracy in classification tasks.},
	urldate = {2023-02-25},
	journal = {Frontiers in Applied Mathematics and Statistics},
	author = {Agerberg, Jens and Ramanujam, Ryan and Scolamiero, Martina and Chachólski, Wojciech},
	year = {2021},
	keywords = {Unread},
}

@article{berwald_mathematics_2019,
	title = {The {Mathematics} of {Quantum}-{Enabled} {Applications} on the {D}-{Wave} {Quantum} {Computer}},
	volume = {66},
	doi = {10.1090/noti1893},
	journal = {Notices of the American Mathematical Society},
	author = {Berwald, Jesse},
	month = jun,
	year = {2019},
	keywords = {Unread},
	pages = {1},
}

@article{cheng_application_2020,
	title = {The {Application} of {Topological} {Data} {Analysis} in {Practice} and {Its} {Effectiveness}},
	volume = {214},
	copyright = {© The Authors, published by EDP Sciences, 2020},
	issn = {2267-1242},
	url = {https://www.e3s-conferences.org/articles/e3sconf/abs/2020/74/e3sconf_ebldm2020_03034/e3sconf_ebldm2020_03034.html},
	doi = {10.1051/e3sconf/202021403034},
	abstract = {Topological Data Analysis(TDA) is a new and fast growing field in data science. TDA provides an approach to analyze data sets and derive their relevant feature out of complex high-dimensional data, which greatly improves the working efficiency in many fields. In this paper, the author mainly discusses some mathematics concepts about topology, methods in TDA and the relation between these topological concepts and data sets (how to apply topological concepts on data). The problems of TDA, mathematical algorithm using in TDA and two application-examples are introduced in this paper. In addition, the advantages, limitations, and the direction of future development of TDA are discussed.},
	language = {en},
	urldate = {2023-02-25},
	journal = {E3S Web of Conferences},
	author = {Cheng, Liang},
	year = {2020},
	note = {Publisher: EDP Sciences},
	keywords = {Unread},
	pages = {03034},
}

@article{topaz_topological_2015,
	title = {Topological {Data} {Analysis} of {Biological} {Aggregation} {Models}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126383},
	doi = {10.1371/journal.pone.0126383},
	abstract = {We apply tools from topological data analysis to two mathematical models inspired by biological aggregations such as bird flocks, fish schools, and insect swarms. Our data consists of numerical simulation output from the models of Vicsek and D'Orsogna. These models are dynamical systems describing the movement of agents who interact via alignment, attraction, and/or repulsion. Each simulation time frame is a point cloud in position-velocity space. We analyze the topological structure of these point clouds, interpreting the persistent homology by calculating the first few Betti numbers. These Betti numbers count connected components, topological circles, and trapped volumes present in the data. To interpret our results, we introduce a visualization that displays Betti numbers over simulation time and topological persistence scale. We compare our topological results to order parameters typically used to quantify the global behavior of aggregations, such as polarization and angular momentum. The topological calculations reveal events and structure not captured by the order parameters.},
	language = {en},
	number = {5},
	urldate = {2023-02-25},
	journal = {PLOS ONE},
	author = {Topaz, Chad M. and Ziegelmeier, Lori and Halverson, Tom},
	month = may,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Algebraic topology, Algorithms, Animal sociality, Biophysical simulations, Dynamical systems, Simulation and modeling, Topology, Unread, Vector spaces},
	pages = {e0126383},
}

@misc{curry_fiber_2019,
	title = {The {Fiber} of the {Persistence} {Map} for {Functions} on the {Interval}},
	url = {http://arxiv.org/abs/1706.06059},
	doi = {10.48550/arXiv.1706.06059},
	abstract = {In this paper we study functions on the interval that have the same persistent homology. By introducing an equivalence relation modeled after topological conjugacy, which we call graph-equivalence, a precise enumeration of functions with the same persistent homology is given, inviting comparisons with Arnold's Calculus of Snakes. The equivalence classes used here are indexed by chiral merge trees, which are binary merge trees where a left-right ordering of the children of each vertex is given. Enumeration of merge trees and chiral merge trees with the same persistence makes essential use of the Elder Rule (a criterion for pairing critical points), which is given a new proof here as well.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Curry, Justin},
	month = jan,
	year = {2019},
	note = {arXiv:1706.06059 [math]},
	keywords = {Mathematics - Algebraic Topology, Unread},
}

@misc{curry_counting_2020,
	title = {Counting {Embedded} {Spheres} with the same {Persistence}},
	url = {http://www.fields.utoronto.ca/talks/Counting-Embedded-Spheres-same-Persistence},
	language = {en},
	urldate = {2023-02-21},
	journal = {Fields Institute for Research in Mathematical Sciences},
	author = {Curry, Justin},
	month = jun,
	year = {2020},
	keywords = {Unread},
}

@phdthesis{poklukar_learning_2022,
	title = {Learning and {Evaluating} the {Geometric} {Structure} of {Representation} {Spaces}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-312723},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-18},
	school = {KTH Royal Institute of Technology},
	author = {Poklukar, Petra},
	year = {2022},
	note = {Publisher: KTH Royal Institute of Technology},
	keywords = {Medium, Unread},
}

@article{doherty_cech_2018,
	title = {The {Cech} complex in {Topological} {Data} {Analysis}},
	url = {https://jdc.math.uwo.ca/TDA/Doherty-Cech-complex.pdf},
	language = {en},
	journal = {University of Western Ontario},
	author = {Doherty, Brandon},
	year = {2018},
	keywords = {Just4Ref, Unread},
}

@misc{chacholski_sf2956_2022,
	title = {{SF2956} {Topological} {Data} {Analysis} lecture notes},
	url = {https://www.kth.se/student/kurser/kurs/SF2956?l=en},
	author = {Chachólski, Wojciech},
	year = {2022},
	keywords = {Unread},
}

@book{hatcher_algebraic_2002,
	address = {Cambridge ; New York},
	title = {Algebraic topology},
	isbn = {978-0-521-79160-1 978-0-521-79540-1},
	publisher = {Cambridge University Press},
	author = {Hatcher, Allen},
	year = {2002},
	keywords = {Algebraic topology, Unread},
}

@misc{robins_resolutions_2000,
	title = {Computational {Topology} at {Multiple} {Resolutions}: {Foundations} and {Applications} to {Fractals} and {Dynamics}},
	language = {en},
	author = {Robins, Vanessa},
	year = {2000},
	keywords = {Just4Ref, Unread},
}

@incollection{goodman_persistent_2008,
	address = {Providence, Rhode Island},
	title = {Persistent homology—a survey},
	volume = {453},
	isbn = {978-0-8218-4239-3 978-0-8218-8132-3},
	url = {http://www.ams.org/conm/453/},
	abstract = {Persistent homology is an algebraic tool for measuring topological features of shapes and functions. It casts the multi-scale organization we frequently observe in nature into a mathematical formalism. Here we give a record of the short history of persistent homology and present its basic concepts. Besides the mathematics we focus on algorithms and mention the various connections to applications, including to biomolecules, biological networks, data analysis, and geometric modeling.},
	language = {en},
	urldate = {2023-02-17},
	booktitle = {Contemporary {Mathematics}},
	publisher = {American Mathematical Society},
	author = {Edelsbrunner, Herbert and Harer, John},
	editor = {Goodman, Jacob E. and Pach, János and Pollack, Richard},
	year = {2008},
	doi = {10.1090/conm/453/08802},
	keywords = {Unread},
	pages = {257--282},
}

@article{ulmer_topological_2019,
	title = {A topological approach to selecting models of biological experiments},
	volume = {14},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0213679},
	doi = {10.1371/journal.pone.0213679},
	language = {en},
	number = {3},
	urldate = {2023-02-17},
	journal = {PLOS ONE},
	author = {Ulmer, M. and Ziegelmeier, Lori and Topaz, Chad M.},
	editor = {Ermentrout, Bard G.},
	month = mar,
	year = {2019},
	keywords = {Just4Ref, Unread},
	pages = {e0213679},
}

@article{fefferman_testing_2016,
	title = {Testing the manifold hypothesis},
	volume = {29},
	issn = {0894-0347, 1088-6834},
	url = {https://www.ams.org/jams/2016-29-04/S0894-0347-2016-00852-4/},
	doi = {10.1090/jams/852},
	language = {en},
	number = {4},
	urldate = {2023-02-08},
	journal = {Journal of the American Mathematical Society},
	author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
	month = feb,
	year = {2016},
	keywords = {Just4Ref, Unread},
	pages = {983--1049},
}

@misc{borkowski_lung_2019,
	title = {Lung and {Colon} {Cancer} {Histopathological} {Image} {Dataset} ({LC25000})},
	url = {http://arxiv.org/abs/1912.12142},
	doi = {10.48550/arXiv.1912.12142},
	abstract = {The field of Machine Learning, a subset of Artificial Intelligence, has led to remarkable advancements in many areas, including medicine. Machine Learning algorithms require large datasets to train computer models successfully. Although there are medical image datasets available, more image datasets are needed from a variety of medical entities, especially cancer pathology. Even more scarce are ML-ready image datasets. To address this need, we created an image dataset (LC25000) with 25,000 color images in 5 classes. Each class contains 5,000 images of the following histologic entities: colon adenocarcinoma, benign colonic tissue, lung adenocarcinoma, lung squamous cell carcinoma, and benign lung tissue. All images are de-identified, HIPAA compliant, validated, and freely available for download to AI researchers.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Borkowski, Andrew A. and Bui, Marilyn M. and Thomas, L. Brannon and Wilson, Catherine P. and DeLand, Lauren A. and Mastorides, Stephen M.},
	month = dec,
	year = {2019},
	note = {arXiv:1912.12142 [cs, eess, q-bio]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Just4Ref, Low, Quantitative Biology - Quantitative Methods, Unread},
}

@book{schervish_theory_1995,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Theory of {Statistics}},
	isbn = {978-1-4612-8708-7 978-1-4612-4250-5},
	url = {http://link.springer.com/10.1007/978-1-4612-4250-5},
	language = {en},
	urldate = {2023-01-28},
	publisher = {Springer},
	author = {Schervish, Mark J.},
	year = {1995},
	doi = {10.1007/978-1-4612-4250-5},
	keywords = {ANOVA, Estimator, Likelihood, Probability distribution, Probability theory, Unread, Variance, bayesian statistics, best fit},
}

@misc{olah_visualizing_2015,
	title = {Visualizing {Representations}: {Deep} {Learning} and {Human} {Beings} - colah's blog},
	url = {http://colah.github.io/posts/2015-01-Visualizing-Representations/},
	urldate = {2023-01-19},
	author = {Olah, Christopher},
	year = {2015},
	keywords = {Just4Ref, Low, Similarity, Unread},
}

@inproceedings{choi_statistical_2021,
	address = {Cham},
	title = {Statistical {Characteristics} of {Deep} {Representations}: {An} {Empirical} {Investigation}},
	isbn = {978-3-030-86383-8},
	abstract = {In this study, the effects of eight representation regularization methods are investigated, including two newly developed rank regularizers (RR). The investigation shows that the statistical characteristics of representations such as correlation, sparsity, and rank can be manipulated as intended, during training. Furthermore, it is possible to improve the baseline performance simply by trying all the representation regularizers and fine-tuning the strength of their effects. In contrast to performance improvement, no consistent relationship between performance and statistical characteristics was observable. The results indicate that manipulation of statistical characteristics can be helpful for improving performance, but only indirectly through its influence on learning dynamics or its tuning effects.},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2021},
	publisher = {Springer International Publishing},
	author = {Choi, Daeyoung and Lee, Kyungeun and Hwang, Duhun and Rhee, Wonjong},
	editor = {Farkaš, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
	year = {2021},
	keywords = {High, Unread},
	pages = {43--55},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language, High, Unread},
}

@article{hensel_survey_2021,
	title = {A {Survey} of {Topological} {Machine} {Learning} {Methods}},
	volume = {4},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.681108},
	abstract = {The last decade saw an enormous boost in the field of computational topology: methods and concepts from algebraic and differential topology, formerly confined to the realm of pure mathematics, have demonstrated their utility in numerous areas such as computational biology personalised medicine, and time-dependent data analysis, to name a few. The newly-emerging domain comprising topology-based techniques is often referred to as topological data analysis (TDA). Next to their applications in the aforementioned areas, TDA methods have also proven to be effective in supporting, enhancing, and augmenting both classical machine learning and deep learning models. In this paper, we review the state of the art of a nascent field we refer to as “topological machine learning,” i.e., the successful symbiosis of topology-based methods and machine learning algorithms, such as deep neural networks. We identify common threads, current applications, and future challenges.},
	urldate = {2023-01-25},
	journal = {Frontiers in Artificial Intelligence},
	author = {Hensel, Felix and Moor, Michael and Rieck, Bastian},
	year = {2021},
	keywords = {Just4Ref, Medium, Unread},
}

@misc{hofer_densified_2021,
	title = {Topologically {Densified} {Distributions}},
	url = {http://arxiv.org/abs/2002.04805},
	doi = {10.48550/arXiv.2002.04805},
	abstract = {We study regularization in the context of small sample-size learning with over-parameterized neural networks. Specifically, we shift focus from architectural properties, such as norms on the network weights, to properties of the internal representations before a linear classifier. Specifically, we impose a topological constraint on samples drawn from the probability measure induced in that space. This provably leads to mass concentration effects around the representations of training instances, i.e., a property beneficial for generalization. By leveraging previous work to impose topological constraints in a neural network setting, we provide empirical evidence (across various vision benchmarks) to support our claim for better generalization.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Hofer, Christoph D. and Graf, Florian and Niethammer, Marc and Kwitt, Roland},
	month = may,
	year = {2021},
	note = {arXiv:2002.04805 [cs, math, stat]},
	keywords = {Main, Reading},
}

@book{taylor_introduction_1973,
	edition = {1},
	title = {Introduction to {Measure} and {Integration}},
	isbn = {978-0-521-09804-5 978-0-511-66247-8},
	url = {https://www.cambridge.org/core/product/identifier/9780511662478/type/book},
	abstract = {This paperback, which comprises the first part of Introduction to Measure and Probability by J. F. C. Kingman and S. J. Taylor, gives a self-contained treatment of the theory of finite measures in general spaces at the undergraduate level. It sets the material out in a form which not only provides an introduction for intending specialists in measure theory but also meets the needs of students of probability. The theory of measure and integration is presented for general spaces, with Lebesgue measure and the Lebesgue integral considered as important examples whose special properties are obtained. The introduction to functional analysis which follows covers the material to probability theory and also the basic theory of L2-spaces, important in modern physics. A large number of examples is included; these form an essential part of the development.},
	urldate = {2023-01-25},
	publisher = {Cambridge University Press},
	author = {Taylor, S. J.},
	month = dec,
	year = {1973},
	doi = {10.1017/CBO9780511662478},
	keywords = {Unread},
}

@book{edelsbrunner_computational_2010,
	address = {Providence, R.I},
	title = {Computational topology: an introduction},
	isbn = {978-0-8218-4925-5},
	shorttitle = {Computational topology},
	publisher = {American Mathematical Society},
	author = {Edelsbrunner, Herbert and Harer, J.},
	year = {2010},
	note = {OCLC: ocn427757156},
	keywords = {Algorithms, Computational complexity, Data processing, Geometry, Topology, Unread},
}

@inproceedings{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	url = {https://proceedings.mlr.press/v97/kornblith19a.html},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {High, Metric, Similarity, Unread},
	pages = {3519--3529},
}

@inproceedings{morcos_insights_2018,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al, 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
	urldate = {2023-01-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
	year = {2018},
	keywords = {High, Just4Ref, Metric, Similarity, Unread},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {Just4Ref, Low, Model, Relative, Unread},
}

@misc{choi_utilizing_2019,
	title = {Utilizing {Class} {Information} for {Deep} {Network} {Representation} {Shaping}},
	url = {http://arxiv.org/abs/1809.09307},
	doi = {10.48550/arXiv.1809.09307},
	abstract = {Statistical characteristics of deep network representations, such as sparsity and correlation, are known to be relevant to the performance and interpretability of deep learning. When a statistical characteristic is desired, often an adequate regularizer can be designed and applied during the training phase. Typically, such a regularizer aims to manipulate a statistical characteristic over all classes together. For classification tasks, however, it might be advantageous to enforce the desired characteristic per class such that different classes can be better distinguished. Motivated by the idea, we design two class-wise regularizers that explicitly utilize class information: class-wise Covariance Regularizer (cw-CR) and class-wise Variance Regularizer (cw-VR). cw-CR targets to reduce the covariance of representations calculated from the same class samples for encouraging feature independence. cw-VR is similar, but variance instead of covariance is targeted to improve feature compactness. For the sake of completeness, their counterparts without using class information, Covariance Regularizer (CR) and Variance Regularizer (VR), are considered together. The four regularizers are conceptually simple and computationally very efficient, and the visualization shows that the regularizers indeed perform distinct representation shaping. In terms of classification performance, significant improvements over the baseline and L1/L2 weight regularization methods were found for 21 out of 22 tasks over popular benchmark datasets. In particular, cw-VR achieved the best performance for 13 tasks including ResNet-32/110.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Choi, Daeyoung and Rhee, Wonjong},
	month = feb,
	year = {2019},
	note = {arXiv:1809.09307 [cs, stat]},
	keywords = {Computer Science - Machine Learning, High, Model, Statistics - Machine Learning, Unread},
}

@misc{hoffman_robust_2019,
	title = {Robust {Learning} with {Jacobian} {Regularization}},
	url = {http://arxiv.org/abs/1908.02729},
	doi = {10.48550/arXiv.1908.02729},
	abstract = {Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Hoffman, Judy and Roberts, Daniel A. and Yaida, Sho},
	month = aug,
	year = {2019},
	note = {arXiv:1908.02729 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Medium, Model, Statistics - Machine Learning, Unread},
}

@misc{cogswell_reducing_2016,
	title = {Reducing {Overfitting} in {Deep} {Networks} by {Decorrelating} {Representations}},
	url = {http://arxiv.org/abs/1511.06068},
	doi = {10.48550/arXiv.1511.06068},
	abstract = {One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Cogswell, Michael and Ahmed, Faruk and Girshick, Ross and Zitnick, Larry and Batra, Dhruv},
	month = jun,
	year = {2016},
	note = {arXiv:1511.06068 [cs, stat]},
	keywords = {Computer Science - Machine Learning, High, Model, Statistics - Machine Learning, Unread},
}

@article{hofmann_kernel_2008,
	title = {Kernel methods in machine learning},
	volume = {36},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/math/0701907},
	doi = {10.1214/009053607000000677},
	abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
	number = {3},
	urldate = {2023-01-20},
	journal = {The Annals of Statistics},
	author = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
	month = jun,
	year = {2008},
	note = {arXiv:math/0701907},
	keywords = {30C40 (Primary) 68T05 (Secondary), Just4Ref, Mathematics - Probability, Mathematics - Statistics Theory, Medium, Relative, Unread},
}

@misc{shalam_self-optimal-transport_2022,
	title = {The {Self}-{Optimal}-{Transport} {Feature} {Transform}},
	url = {http://arxiv.org/abs/2204.03065},
	abstract = {The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the set of features of a data instance to facilitate downstream matching or grouping related tasks. The transformed set encodes a rich representation of high order relations between the instance features. Distances between transformed features capture their direct original similarity and their third party agreement regarding similarity to other features in the set. A particular min-cost-max-flow fractional matching problem, whose entropy regularized version can be approximated by an optimal transport (OT) optimization, results in our transductive transform which is efficient, differentiable, equivariant, parameterless and probabilistically interpretable. Empirically, the transform is highly effective and flexible in its use, consistently improving networks it is inserted into, in a variety of tasks and training schemes. We demonstrate its merits through the problem of unsupervised clustering and its efficiency and wide applicability for few-shot-classification, with state-of-the-art results, and large-scale person re-identification.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Shalam, Daniel and Korman, Simon},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03065 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Just4Ref, Low, Relative, Unread},
}

@inproceedings{lenc_understanding_2015,
	title = {Understanding image representations by measuring their equivariance and equivalence},
	doi = {10.1109/CVPR.2015.7298701},
	abstract = {Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lenc, Karel and Vedaldi, Andrea},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Computer vision, Convolutional codes, Feature extraction, Histograms, Image reconstruction, Image representation, Just4Ref, Medium, Neural networks, Similarity, Stiching, Unread},
	pages = {991--999},
}

@inproceedings{wang_towards_2018,
	title = {Towards {Understanding} {Learning} {Representations}: {To} {What} {Extent} {Do} {Different} {Neural} {Networks} {Learn} the {Same} {Representation}},
	volume = {31},
	shorttitle = {Towards {Understanding} {Learning} {Representations}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/5fc34ed307aac159a30d81181c99847e-Abstract.html},
	abstract = {It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations.  We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.},
	urldate = {2023-01-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Hu, Zhiqiang and Wu, Yue and He, Kun and Hopcroft, John},
	year = {2018},
	keywords = {High, Just4Ref, Similarity, Unread},
}

@misc{gygli_towards_2020,
	title = {Towards {Reusable} {Network} {Components} by {Learning} {Compatible} {Representations}},
	url = {http://arxiv.org/abs/2004.03898},
	doi = {10.48550/arXiv.2004.03898},
	abstract = {This paper proposes to make a first step towards compatible and hence reusable network components. Rather than training networks for different tasks independently, we adapt the training process to produce network components that are compatible across tasks. In particular, we split a network into two components, a features extractor and a target task head, and propose various approaches to accomplish compatibility between them. We systematically analyse these approaches on the task of image classification on standard datasets. We demonstrate that we can produce components which are directly compatible without any fine-tuning or compromising accuracy on the original tasks. Afterwards, we demonstrate the use of compatible components on three applications: Unsupervised domain adaptation, transferring classifiers across feature extractors with different architectures, and increasing the computational efficiency of transfer learning.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Gygli, Michael and Uijlings, Jasper and Ferrari, Vittorio},
	month = dec,
	year = {2020},
	note = {arXiv:2004.03898 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Medium, Statistics - Machine Learning, Stiching, Unread},
}

@misc{csiszarik_similarity_2021,
	title = {Similarity and {Matching} of {Neural} {Network} {Representations}},
	url = {http://arxiv.org/abs/2110.14633},
	doi = {10.48550/arXiv.2110.14633},
	abstract = {We employ a toolset -- dubbed Dr. Frankenstein -- to analyse the similarity of representations in deep neural networks. With this toolset, we aim to match the activations on given layers of two trained neural networks by joining them with a stitching layer. We demonstrate that the inner representations emerging in deep convolutional neural networks with the same architecture but different initializations can be matched with a surprisingly high degree of accuracy even with a single, affine stitching layer. We choose the stitching layer from several possible classes of linear transformations and investigate their performance and properties. The task of matching representations is closely related to notions of similarity. Using this toolset, we also provide a novel viewpoint on the current line of research regarding similarity indices of neural network representations: the perspective of the performance on a task.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Csiszárik, Adrián and Kőrösi-Szabó, Péter and Matszangosz, Ákos K. and Papp, Gergely and Varga, Dániel},
	month = oct,
	year = {2021},
	note = {arXiv:2110.14633 [cs]},
	keywords = {Computer Science - Machine Learning, High, Similarity, Stiching, Unread},
}

@inproceedings{bansal_revisiting_2021,
	title = {Revisiting {Model} {Stitching} to {Compare} {Neural} {Representations}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/01ded4259d101feb739b06c399e9cd9c-Abstract.html},
	urldate = {2023-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
	year = {2021},
	keywords = {High, Just4Ref, Stiching, Unread},
	pages = {225--236},
}

@misc{li_convergent_2016,
	title = {Convergent {Learning}: {Do} different neural networks learn the same representations?},
	shorttitle = {Convergent {Learning}},
	url = {http://arxiv.org/abs/1511.07543},
	doi = {10.48550/arXiv.1511.07543},
	abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	month = feb,
	year = {2016},
	note = {arXiv:1511.07543 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Just4Ref, Low, Similarity, Unread},
}

@inproceedings{vulic_are_2020,
	address = {Online},
	title = {Are {All} {Good} {Word} {Vector} {Spaces} {Isomorphic}?},
	url = {https://aclanthology.org/2020.emnlp-main.257},
	doi = {10.18653/v1/2020.emnlp-main.257},
	abstract = {Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. “under-training”).},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Vulić, Ivan and Ruder, Sebastian and Søgaard, Anders},
	month = nov,
	year = {2020},
	keywords = {Just4Ref, Low, Similarity, Unread},
	pages = {3178--3192},
}

@inproceedings{soudry_implicit_2022,
	title = {The {Implicit} {Bias} of {Gradient} {Descent} on {Separable} {Data}},
	url = {https://openreview.net/forum?id=r1q7n9gAb},
	abstract = {We show that gradient descent on an unregularized logistic regression problem, for almost all separable datasets, converges to the same direction as the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
	language = {en},
	urldate = {2023-01-19},
	author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Srebro, Nathan},
	month = feb,
	year = {2022},
	keywords = {Just4Ref, Low, Unread},
}

@misc{moschella_relative_2022,
	title = {Relative representations enable zero-shot latent space communication},
	url = {http://arxiv.org/abs/2209.15430},
	doi = {10.48550/arXiv.2209.15430},
	abstract = {Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, distinct latent spaces typically differ by an unknown quasi-isometric transformation: that is, in each space, the distances between the encodings do not change. In this work, we propose to adopt pairwise similarities as an alternative data representation, that can be used to enforce the desired invariance without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, latent isometry invariance, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Moschella, Luca and Maiorca, Valentino and Fumero, Marco and Norelli, Antonio and Locatello, Francesco and Rodolà, Emanuele},
	month = sep,
	year = {2022},
	note = {arXiv:2209.15430 [cs]},
	keywords = {Main, Reading},
}

@misc{kashlak_probability_2022,
	title = {Probability and {Measure}},
	url = {https://sites.ualberta.ca/~kashlak/data/stat571.pdf},
	abstract = {Course notes for STAT 571},
	language = {eng},
	author = {Kashlak, Adam B.},
	year = {2022},
}

@misc{zhang_convergence_2022,
	title = {On the {Convergence} of {Optimizing} {Persistent}-{Homology}-{Based} {Losses}},
	url = {http://arxiv.org/abs/2206.02946},
	doi = {10.48550/arXiv.2206.02946},
	abstract = {Topological loss based on persistent homology has shown promise in various applications. A topological loss enforces the model to achieve certain desired topological property. Despite its empirical success, less is known about the optimization behavior of the loss. In fact, the topological loss involves combinatorial configurations that may oscillate during optimization. In this paper, we introduce a general purpose regularized topology-aware loss. We propose a novel regularization term and also modify existing topological loss. These contributions lead to a new loss function that not only enforces the model to have desired topological behavior, but also achieves satisfying convergence behavior. Our main theoretical result guarantees that the loss can be optimized efficiently, under mild assumptions.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Zhang, Yikai and Yao, Jiachen and Wang, Yusu and Chen, Chao},
	month = jun,
	year = {2022},
	note = {arXiv:2206.02946 [cs, stat]},
	keywords = {Just4Ref, Medium, Unread},
}

@misc{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	doi = {10.48550/arXiv.1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = feb,
	year = {2017},
	note = {arXiv:1611.03530 [cs]},
	keywords = {Computer Science - Machine Learning, Just4Ref, Low, Unread},
}

@phdthesis{leitao_topological_2021,
	title = {Topological {Expressiveness} of {Neural} {Networks}},
	url = {https://run.unl.pt/bitstream/10362/129615/1/TAA0115.pdf},
	abstract = {Given a neural network, how many di erent problems can it solve? This important and open
question in deep learning is usually referred to as the problem of the expressive power of a
neural network. Previous research has tackled this issue through statistical and geometrical
methods. This work proposes a new method based on a topological perspective.
Topology is the  eld of mathematics aimed at describing spaces and functions through
robust characterizing features. Topological Data Analysis is the young  eld developed to
extract topological insight from data.
We  rst show that topological features of the decision boundary describe the closest
notion of the intrinsic complexity of a classi cation problem. These topological features divide
classi cation problems into several equivalence classes. Linear-separability is an example of
such a class. We establish the topological expressive power of a network architecture as the
number of di erent topological classes it is able to express.
Being a novel work in a young research  eld, most of the thesis is devoted to developing
this perspective and creating the tools required. The main objective of this thesis is to tackle
neural network’s understanding in general and architecture selection in particular, through a
novel approach.
Our results show that topological expressiveness has a complex correlation with many features in a neural network’s architecture depending weakly on the total number of parameters.
Some of our results recapitulate previous research on geometrical properties, while others are
unique to this novel topological point of view, sometimes challenging previous research.},
	language = {eng},
	author = {Leitão, António},
	year = {2021},
	keywords = {Medium, Unread},
}

@misc{moor_topological_2021,
	title = {Topological {Autoencoders}},
	url = {http://arxiv.org/abs/1906.00722},
	doi = {10.48550/arXiv.1906.00722},
	abstract = {We propose a novel approach for preserving topological structures of the input space in latent representations of autoencoders. Using persistent homology, a technique from topological data analysis, we calculate topological signatures of both the input and latent space to derive a topological loss term. Under weak theoretical assumptions, we construct this loss in a differentiable manner, such that the encoding learns to retain multi-scale connectivity information. We show that our approach is theoretically well-founded and that it exhibits favourable latent representations on a synthetic manifold as well as on real-world image data sets, while preserving low reconstruction errors.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Moor, Michael and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
	month = may,
	year = {2021},
	note = {arXiv:1906.00722 [cs, math, stat]},
	keywords = {High, Unread},
}

@misc{carriere_optimizing_2021,
	title = {Optimizing persistent homology based functions},
	url = {http://arxiv.org/abs/2010.08356},
	doi = {10.48550/arXiv.2010.08356},
	abstract = {Solving optimization tasks based on functions and losses with a topological flavor is a very active, growing field of research in data science and Topological Data Analysis, with applications in non-convex optimization, statistics and machine learning. However, the approaches proposed in the literature are usually anchored to a specific application and/or topological construction, and do not come with theoretical guarantees. To address this issue, we study the differentiability of a general map associated with the most common topological construction, that is, the persistence map. Building on real analytic geometry arguments, we propose a general framework that allows us to define and compute gradients for persistence-based functions in a very simple way. We also provide a simple, explicit and sufficient condition for convergence of stochastic subgradient methods for such functions. This result encompasses all the constructions and applications of topological optimization in the literature. Finally, we provide associated code, that is easy to handle and to mix with other non-topological methods and constraints, as well as some experiments showcasing the versatility of our approach.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Carrière, Mathieu and Chazal, Frédéric and Glisse, Marc and Ike, Yuichi and Kannan, Hariprasad},
	month = feb,
	year = {2021},
	note = {arXiv:2010.08356 [cs, math]},
	keywords = {High, Unread},
}

@inproceedings{poklukar_delaunay_2022,
	title = {Delaunay {Component} {Analysis} for {Evaluation} of {Data} {Representations}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-312715},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-18},
	author = {Poklukar, Petra and Polianskii, Vladislav and Varava, Anastasiia and Pokorny, Florian T. and Kragic, Danica},
	year = {2022},
	keywords = {High, Unread},
}

@article{rieck_neural_2019,
	title = {Neural {Persistence}: {A} {Complexity} {Measure} for {Deep} {Neural} {Networks} {Using} {Algebraic} {Topology}},
	shorttitle = {Neural {Persistence}},
	url = {http://arxiv.org/abs/1812.09764},
	doi = {10.3929/ethz-b-000327207},
	abstract = {While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization. Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.},
	urldate = {2023-01-19},
	author = {Rieck, Bastian and Togninalli, Matteo and Bock, Christian and Moor, Michael and Horn, Max and Gumbsch, Thomas and Borgwardt, Karsten},
	month = feb,
	year = {2019},
	note = {arXiv:1812.09764 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Just4Ref, Low, Mathematics - Algebraic Topology, Statistics - Machine Learning, Unread},
	pages = {25 p.},
}

@misc{chen_topological_2018,
	title = {A {Topological} {Regularizer} for {Classifiers} via {Persistent} {Homology}},
	url = {http://arxiv.org/abs/1806.10714},
	doi = {10.48550/arXiv.1806.10714},
	abstract = {Regularization plays a crucial role in supervised learning. Most existing methods enforce a global regularization in a structure agnostic manner. In this paper, we initiate a new direction and propose to enforce the structural simplicity of the classification boundary by regularizing over its topological complexity. In particular, our measurement of topological complexity incorporates the importance of topological features (e.g., connected components, handles, and so on) in a meaningful manner, and provides a direct control over spurious topological structures. We incorporate the new measurement as a topological penalty in training classifiers. We also pro- pose an efficient algorithm to compute the gradient of such penalty. Our method pro- vides a novel way to topologically simplify the global structure of the model, without having to sacrifice too much of the flexibility of the model. We demonstrate the effectiveness of our new topological regularizer on a range of synthetic and real-world datasets.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Chen, Chao and Ni, Xiuyan and Bai, Qinxun and Wang, Yusu},
	month = oct,
	year = {2018},
	note = {arXiv:1806.10714 [cs, stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Just4Ref, Medium, Statistics - Machine Learning, Unread},
}

@misc{belharbi_neural_2017,
	title = {Neural {Networks} {Regularization} {Through} {Class}-wise {Invariant} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1709.01867},
	doi = {10.48550/arXiv.1709.01867},
	abstract = {Training deep neural networks is known to require a large number of training samples. However, in many applications only few training samples are available. In this work, we tackle the issue of training neural networks for classification task when few training samples are available. We attempt to solve this issue by proposing a new regularization term that constrains the hidden layers of a network to learn class-wise invariant representations. In our regularization framework, learning invariant representations is generalized to the class membership where samples with the same class should have the same representation. Numerical experiments over MNIST and its variants showed that our proposal helps improving the generalization of neural network particularly when trained with few samples. We provide the source code of our framework https://github.com/sbelharbi/learning-class-invariant-features .},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Belharbi, Soufiane and Chatelain, Clément and Hérault, Romain and Adam, Sébastien},
	month = dec,
	year = {2017},
	note = {arXiv:1709.01867 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Just4Ref, Medium, Statistics - Machine Learning, Unread},
}

@misc{verma_manifold_2019,
	title = {Manifold {Mixup}: {Better} {Representations} by {Interpolating} {Hidden} {States}},
	shorttitle = {Manifold {Mixup}},
	url = {http://arxiv.org/abs/1806.05236},
	doi = {10.48550/arXiv.1806.05236},
	abstract = {Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Courville, Aaron and Lopez-Paz, David and Bengio, Yoshua},
	month = may,
	year = {2019},
	note = {arXiv:1806.05236 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Just4Ref, Medium, Statistics - Machine Learning, Unread},
}

@inproceedings{glorot_deep_2011,
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	month = jun,
	year = {2011},
	note = {ISSN: 1938-7228},
	keywords = {Just4Ref, Low, Unread},
	pages = {315--323},
}

@misc{hofer_connectivity-optimized_2019,
	title = {Connectivity-{Optimized} {Representation} {Learning} via {Persistent} {Homology}},
	url = {http://arxiv.org/abs/1906.09003},
	doi = {10.48550/arXiv.1906.09003},
	abstract = {We study the problem of learning representations with controllable connectivity properties. This is beneficial in situations when the imposed structure can be leveraged upstream. In particular, we control the connectivity of an autoencoder's latent space via a novel type of loss, operating on information from persistent homology. Under mild conditions, this loss is differentiable and we present a theoretical analysis of the properties induced by the loss. We choose one-class learning as our upstream task and demonstrate that the imposed structure enables informed parameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on computer vision data, these one-class models exhibit competitive performance and, in a low sample size regime, outperform other methods by a large margin. Notably, our results indicate that a single autoencoder, trained on auxiliary (unlabeled) data, yields a mapping into latent space that can be reused across datasets for one-class learning.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Hofer, Christoph and Kwitt, Roland and Dixit, Mandar and Niethammer, Marc},
	month = jun,
	year = {2019},
	note = {arXiv:1906.09003 [cs, math, stat]},
	keywords = {High, Unread},
}
