
@inproceedings{schonenberger_witness_2022,
	title = {Witness {Autoencoder}: {Shaping} the {Latent} {Space} with {Witness} {Complexes}},
	shorttitle = {Witness {Autoencoder}},
	url = {https://openreview.net/forum?id=1gQfXt_U5a-},
	abstract = {We present a Witness Autoencoder (W-AE) – an autoencoder that captures geodesic distances of the data in the latent space. Our algorithm uses witness complexes to compute geodesic distance approximations on a mini-batch level, and leverages topological information from the entire dataset while performing batch-wise approximations. This way, our method allows to capture the global structure of the data even with a small batch size, which is beneficial for large-scale real-world data. We show that our method captures the structure of the manifold more accurately than the recently introduced topological autoencoder (TopoAE).},
	language = {en},
	urldate = {2023-01-25},
	booktitle = {Witness {Autoencoder}: {Shaping} the {Latent} {Space} with {Witness} {Complexes}},
	author = {Schönenberger, Simon Till and Varava, Anastasiia and Polianskii, Vladislav and Chung, Jen Jen and Kragic, Danica and Siegwart, Roland},
	month = jul,
	year = {2022},
	keywords = {High, Unread},
}

@misc{curry_fiber_2019,
	title = {The {Fiber} of the {Persistence} {Map} for {Functions} on the {Interval}},
	url = {http://arxiv.org/abs/1706.06059},
	doi = {10.48550/arXiv.1706.06059},
	abstract = {In this paper we study functions on the interval that have the same persistent homology. By introducing an equivalence relation modeled after topological conjugacy, which we call graph-equivalence, a precise enumeration of functions with the same persistent homology is given, inviting comparisons with Arnold's Calculus of Snakes. The equivalence classes used here are indexed by chiral merge trees, which are binary merge trees where a left-right ordering of the children of each vertex is given. Enumeration of merge trees and chiral merge trees with the same persistence makes essential use of the Elder Rule (a criterion for pairing critical points), which is given a new proof here as well.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Curry, Justin},
	month = jan,
	year = {2019},
	note = {arXiv:1706.06059 [math]},
	keywords = {Mathematics - Algebraic Topology, Unread},
}

@misc{curry_counting_2020,
	title = {Counting {Embedded} {Spheres} with the same {Persistence}},
	url = {http://www.fields.utoronto.ca/talks/Counting-Embedded-Spheres-same-Persistence},
	language = {en},
	urldate = {2023-02-21},
	journal = {Fields Institute for Research in Mathematical Sciences},
	author = {Curry, Justin},
	month = jun,
	year = {2020},
	keywords = {Unread},
}

@article{cohen-steiner_stability_2007,
	title = {Stability of {Persistence} {Diagrams}},
	volume = {37},
	issn = {0179-5376, 1432-0444},
	url = {http://link.springer.com/10.1007/s00454-006-1276-5},
	doi = {10.1007/s00454-006-1276-5},
	language = {en},
	number = {1},
	urldate = {2023-02-21},
	journal = {Discrete \& Computational Geometry},
	author = {Cohen-Steiner, David and Edelsbrunner, Herbert and Harer, John},
	month = jan,
	year = {2007},
	keywords = {Unread},
	pages = {103--120},
}

@phdthesis{poklukar_learning_2022,
	title = {Learning and {Evaluating} the {Geometric} {Structure} of {Representation} {Spaces}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-312723},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-18},
	school = {KTH Royal Institute of Technology},
	author = {Poklukar, Petra},
	year = {2022},
	note = {Publisher: KTH Royal Institute of Technology},
	keywords = {Medium, Unread},
}

@article{doherty_cech_2018,
	title = {The {Cech} complex in {Topological} {Data} {Analysis}},
	url = {https://jdc.math.uwo.ca/TDA/Doherty-Cech-complex.pdf},
	language = {en},
	journal = {University of Western Ontario},
	author = {Doherty, Brandon},
	year = {2018},
	keywords = {Just4Ref, Unread},
}

@misc{chacholski_sf2956_2022,
	title = {{SF2956} {Topological} {Data} {Analysis} lecture notes},
	url = {https://www.kth.se/student/kurser/kurs/SF2956?l=en},
	author = {Chachólski, Wojciech},
	year = {2022},
	keywords = {Unread},
}

@book{hatcher_algebraic_2002,
	address = {Cambridge ; New York},
	title = {Algebraic topology},
	isbn = {978-0-521-79160-1 978-0-521-79540-1},
	publisher = {Cambridge University Press},
	author = {Hatcher, Allen},
	year = {2002},
	keywords = {Algebraic topology, Unread},
}

@misc{robins_resolutions_2000,
	title = {Computational {Topology} at {Multiple} {Resolutions}: {Foundations} and {Applications} to {Fractals} and {Dynamics}},
	language = {en},
	author = {Robins, Vanessa},
	year = {2000},
	keywords = {Just4Ref, Unread},
}

@incollection{goodman_persistent_2008,
	address = {Providence, Rhode Island},
	title = {Persistent homology—a survey},
	volume = {453},
	isbn = {978-0-8218-4239-3 978-0-8218-8132-3},
	url = {http://www.ams.org/conm/453/},
	abstract = {Persistent homology is an algebraic tool for measuring topological features of shapes and functions. It casts the multi-scale organization we frequently observe in nature into a mathematical formalism. Here we give a record of the short history of persistent homology and present its basic concepts. Besides the mathematics we focus on algorithms and mention the various connections to applications, including to biomolecules, biological networks, data analysis, and geometric modeling.},
	language = {en},
	urldate = {2023-02-17},
	booktitle = {Contemporary {Mathematics}},
	publisher = {American Mathematical Society},
	author = {Edelsbrunner, Herbert and Harer, John},
	editor = {Goodman, Jacob E. and Pach, János and Pollack, Richard},
	year = {2008},
	doi = {10.1090/conm/453/08802},
	keywords = {Unread},
	pages = {257--282},
}

@article{ulmer_topological_2019,
	title = {A topological approach to selecting models of biological experiments},
	volume = {14},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0213679},
	doi = {10.1371/journal.pone.0213679},
	language = {en},
	number = {3},
	urldate = {2023-02-17},
	journal = {PLOS ONE},
	author = {Ulmer, M. and Ziegelmeier, Lori and Topaz, Chad M.},
	editor = {Ermentrout, Bard G.},
	month = mar,
	year = {2019},
	keywords = {Just4Ref, Unread},
	pages = {e0213679},
}

@article{fefferman_testing_2016,
	title = {Testing the manifold hypothesis},
	volume = {29},
	issn = {0894-0347, 1088-6834},
	url = {https://www.ams.org/jams/2016-29-04/S0894-0347-2016-00852-4/},
	doi = {10.1090/jams/852},
	language = {en},
	number = {4},
	urldate = {2023-02-08},
	journal = {Journal of the American Mathematical Society},
	author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
	month = feb,
	year = {2016},
	keywords = {Just4Ref, Unread},
	pages = {983--1049},
}

@misc{borkowski_lung_2019,
	title = {Lung and {Colon} {Cancer} {Histopathological} {Image} {Dataset} ({LC25000})},
	url = {http://arxiv.org/abs/1912.12142},
	doi = {10.48550/arXiv.1912.12142},
	abstract = {The field of Machine Learning, a subset of Artificial Intelligence, has led to remarkable advancements in many areas, including medicine. Machine Learning algorithms require large datasets to train computer models successfully. Although there are medical image datasets available, more image datasets are needed from a variety of medical entities, especially cancer pathology. Even more scarce are ML-ready image datasets. To address this need, we created an image dataset (LC25000) with 25,000 color images in 5 classes. Each class contains 5,000 images of the following histologic entities: colon adenocarcinoma, benign colonic tissue, lung adenocarcinoma, lung squamous cell carcinoma, and benign lung tissue. All images are de-identified, HIPAA compliant, validated, and freely available for download to AI researchers.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Borkowski, Andrew A. and Bui, Marilyn M. and Thomas, L. Brannon and Wilson, Catherine P. and DeLand, Lauren A. and Mastorides, Stephen M.},
	month = dec,
	year = {2019},
	note = {arXiv:1912.12142 [cs, eess, q-bio]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Just4Ref, Low, Quantitative Biology - Quantitative Methods, Unread},
}

@misc{guo_zero-shot_2022,
	title = {Zero-{Shot} and {Few}-{Shot} {Learning} for {Lung} {Cancer} {Multi}-{Label} {Classification} using {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2205.15290},
	doi = {10.48550/arXiv.2205.15290},
	abstract = {Lung cancer is the leading cause of cancer-related death worldwide. Lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most common histologic subtypes of non-small-cell lung cancer (NSCLC). Histology is an essential tool for lung cancer diagnosis. Pathologists make classifications according to the dominant subtypes. Although morphology remains the standard for diagnosis, significant tool needs to be developed to elucidate the diagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT) model to classify multiple label lung cancer on histologic slices (from dataset LC25000), in both Zero-Shot and Few-Shot settings. Then we compare the performance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall, sensitivity and specificity. Our study show that the pre-trained ViT model has a good performance in Zero-Shot setting, a competitive accuracy (\$99.87{\textbackslash}\%\$) in Few-Shot setting (\{epoch = 1\}) and an optimal result (\$100.00{\textbackslash}\%\$ on both validation set and test set) in Few-Shot seeting (\{epoch = 5\}).},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Guo, Fu-Ming and Fan, Yingfang},
	month = may,
	year = {2022},
	note = {arXiv:2205.15290 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Low, Unread},
}

@book{schervish_theory_1995,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Theory of {Statistics}},
	isbn = {978-1-4612-8708-7 978-1-4612-4250-5},
	url = {http://link.springer.com/10.1007/978-1-4612-4250-5},
	language = {en},
	urldate = {2023-01-28},
	publisher = {Springer},
	author = {Schervish, Mark J.},
	year = {1995},
	doi = {10.1007/978-1-4612-4250-5},
	keywords = {ANOVA, Estimator, Likelihood, Probability distribution, Probability theory, Unread, Variance, bayesian statistics, best fit},
}

@misc{olah_visualizing_2015,
	title = {Visualizing {Representations}: {Deep} {Learning} and {Human} {Beings} - colah's blog},
	url = {http://colah.github.io/posts/2015-01-Visualizing-Representations/},
	urldate = {2023-01-19},
	author = {Olah, Christopher},
	year = {2015},
	keywords = {Just4Ref, Low, Similarity, Unread},
}

@inproceedings{choi_statistical_2021,
	address = {Cham},
	title = {Statistical {Characteristics} of {Deep} {Representations}: {An} {Empirical} {Investigation}},
	isbn = {978-3-030-86383-8},
	abstract = {In this study, the effects of eight representation regularization methods are investigated, including two newly developed rank regularizers (RR). The investigation shows that the statistical characteristics of representations such as correlation, sparsity, and rank can be manipulated as intended, during training. Furthermore, it is possible to improve the baseline performance simply by trying all the representation regularizers and fine-tuning the strength of their effects. In contrast to performance improvement, no consistent relationship between performance and statistical characteristics was observable. The results indicate that manipulation of statistical characteristics can be helpful for improving performance, but only indirectly through its influence on learning dynamics or its tuning effects.},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2021},
	publisher = {Springer International Publishing},
	author = {Choi, Daeyoung and Lee, Kyungeun and Hwang, Duhun and Rhee, Wonjong},
	editor = {Farkaš, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
	year = {2021},
	keywords = {High, Unread},
	pages = {43--55},
}

@inproceedings{keung_multilingual_2020,
	address = {Online},
	title = {The {Multilingual} {Amazon} {Reviews} {Corpus}},
	url = {https://aclanthology.org/2020.emnlp-main.369},
	doi = {10.18653/v1/2020.emnlp-main.369},
	abstract = {We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., `books', `appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20\% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Keung, Phillip and Lu, Yichao and Szarvas, György and Smith, Noah A.},
	month = nov,
	year = {2020},
	keywords = {Just4Ref, Low, Unread},
	pages = {4563--4568},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	keywords = {Just4Ref, Medium, Unread},
	pages = {4171--4186},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language, High, Unread},
}

@misc{dosovitskiy_vit_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Just4Ref, Low, Unread},
}

@article{hensel_survey_2021,
	title = {A {Survey} of {Topological} {Machine} {Learning} {Methods}},
	volume = {4},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.681108},
	abstract = {The last decade saw an enormous boost in the field of computational topology: methods and concepts from algebraic and differential topology, formerly confined to the realm of pure mathematics, have demonstrated their utility in numerous areas such as computational biology personalised medicine, and time-dependent data analysis, to name a few. The newly-emerging domain comprising topology-based techniques is often referred to as topological data analysis (TDA). Next to their applications in the aforementioned areas, TDA methods have also proven to be effective in supporting, enhancing, and augmenting both classical machine learning and deep learning models. In this paper, we review the state of the art of a nascent field we refer to as “topological machine learning,” i.e., the successful symbiosis of topology-based methods and machine learning algorithms, such as deep neural networks. We identify common threads, current applications, and future challenges.},
	urldate = {2023-01-25},
	journal = {Frontiers in Artificial Intelligence},
	author = {Hensel, Felix and Moor, Michael and Rieck, Bastian},
	year = {2021},
	keywords = {Just4Ref, Medium, Unread},
}

@misc{hofer_densified_2021,
	title = {Topologically {Densified} {Distributions}},
	url = {http://arxiv.org/abs/2002.04805},
	doi = {10.48550/arXiv.2002.04805},
	abstract = {We study regularization in the context of small sample-size learning with over-parameterized neural networks. Specifically, we shift focus from architectural properties, such as norms on the network weights, to properties of the internal representations before a linear classifier. Specifically, we impose a topological constraint on samples drawn from the probability measure induced in that space. This provably leads to mass concentration effects around the representations of training instances, i.e., a property beneficial for generalization. By leveraging previous work to impose topological constraints in a neural network setting, we provide empirical evidence (across various vision benchmarks) to support our claim for better generalization.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Hofer, Christoph D. and Graf, Florian and Niethammer, Marc and Kwitt, Roland},
	month = may,
	year = {2021},
	note = {arXiv:2002.04805 [cs, math, stat]},
	keywords = {Main, Reading},
}

@book{taylor_introduction_1973,
	edition = {1},
	title = {Introduction to {Measure} and {Integration}},
	isbn = {978-0-521-09804-5 978-0-511-66247-8},
	url = {https://www.cambridge.org/core/product/identifier/9780511662478/type/book},
	abstract = {This paperback, which comprises the first part of Introduction to Measure and Probability by J. F. C. Kingman and S. J. Taylor, gives a self-contained treatment of the theory of finite measures in general spaces at the undergraduate level. It sets the material out in a form which not only provides an introduction for intending specialists in measure theory but also meets the needs of students of probability. The theory of measure and integration is presented for general spaces, with Lebesgue measure and the Lebesgue integral considered as important examples whose special properties are obtained. The introduction to functional analysis which follows covers the material to probability theory and also the basic theory of L2-spaces, important in modern physics. A large number of examples is included; these form an essential part of the development.},
	urldate = {2023-01-25},
	publisher = {Cambridge University Press},
	author = {Taylor, S. J.},
	month = dec,
	year = {1973},
	doi = {10.1017/CBO9780511662478},
	keywords = {Unread},
}

@book{edelsbrunner_computational_2010,
	address = {Providence, R.I},
	title = {Computational topology: an introduction},
	isbn = {978-0-8218-4925-5},
	shorttitle = {Computational topology},
	publisher = {American Mathematical Society},
	author = {Edelsbrunner, Herbert and Harer, J.},
	year = {2010},
	note = {OCLC: ocn427757156},
	keywords = {Algorithms, Computational complexity, Data processing, Geometry, Topology, Unread},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	doi = {10.48550/arXiv.2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Just4Ref, Low, Unread},
}

@inproceedings{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	url = {https://proceedings.mlr.press/v97/kornblith19a.html},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	keywords = {High, Metric, Similarity, Unread},
	pages = {3519--3529},
}

@inproceedings{morcos_insights_2018,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al, 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
	urldate = {2023-01-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
	year = {2018},
	keywords = {High, Just4Ref, Metric, Similarity, Unread},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {Just4Ref, Low, Model, Relative, Unread},
}

@misc{choi_utilizing_2019,
	title = {Utilizing {Class} {Information} for {Deep} {Network} {Representation} {Shaping}},
	url = {http://arxiv.org/abs/1809.09307},
	doi = {10.48550/arXiv.1809.09307},
	abstract = {Statistical characteristics of deep network representations, such as sparsity and correlation, are known to be relevant to the performance and interpretability of deep learning. When a statistical characteristic is desired, often an adequate regularizer can be designed and applied during the training phase. Typically, such a regularizer aims to manipulate a statistical characteristic over all classes together. For classification tasks, however, it might be advantageous to enforce the desired characteristic per class such that different classes can be better distinguished. Motivated by the idea, we design two class-wise regularizers that explicitly utilize class information: class-wise Covariance Regularizer (cw-CR) and class-wise Variance Regularizer (cw-VR). cw-CR targets to reduce the covariance of representations calculated from the same class samples for encouraging feature independence. cw-VR is similar, but variance instead of covariance is targeted to improve feature compactness. For the sake of completeness, their counterparts without using class information, Covariance Regularizer (CR) and Variance Regularizer (VR), are considered together. The four regularizers are conceptually simple and computationally very efficient, and the visualization shows that the regularizers indeed perform distinct representation shaping. In terms of classification performance, significant improvements over the baseline and L1/L2 weight regularization methods were found for 21 out of 22 tasks over popular benchmark datasets. In particular, cw-VR achieved the best performance for 13 tasks including ResNet-32/110.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Choi, Daeyoung and Rhee, Wonjong},
	month = feb,
	year = {2019},
	note = {arXiv:1809.09307 [cs, stat]},
	keywords = {Computer Science - Machine Learning, High, Model, Statistics - Machine Learning, Unread},
}

@misc{hoffman_robust_2019,
	title = {Robust {Learning} with {Jacobian} {Regularization}},
	url = {http://arxiv.org/abs/1908.02729},
	doi = {10.48550/arXiv.1908.02729},
	abstract = {Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Hoffman, Judy and Roberts, Daniel A. and Yaida, Sho},
	month = aug,
	year = {2019},
	note = {arXiv:1908.02729 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Medium, Model, Statistics - Machine Learning, Unread},
}

@misc{cogswell_reducing_2016,
	title = {Reducing {Overfitting} in {Deep} {Networks} by {Decorrelating} {Representations}},
	url = {http://arxiv.org/abs/1511.06068},
	doi = {10.48550/arXiv.1511.06068},
	abstract = {One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Cogswell, Michael and Ahmed, Faruk and Girshick, Ross and Zitnick, Larry and Batra, Dhruv},
	month = jun,
	year = {2016},
	note = {arXiv:1511.06068 [cs, stat]},
	keywords = {Computer Science - Machine Learning, High, Model, Statistics - Machine Learning, Unread},
}

@article{hofmann_kernel_2008,
	title = {Kernel methods in machine learning},
	volume = {36},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/math/0701907},
	doi = {10.1214/009053607000000677},
	abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
	number = {3},
	urldate = {2023-01-20},
	journal = {The Annals of Statistics},
	author = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
	month = jun,
	year = {2008},
	note = {arXiv:math/0701907},
	keywords = {30C40 (Primary) 68T05 (Secondary), Just4Ref, Mathematics - Probability, Mathematics - Statistics Theory, Medium, Relative, Unread},
}

@misc{shalam_self-optimal-transport_2022,
	title = {The {Self}-{Optimal}-{Transport} {Feature} {Transform}},
	url = {http://arxiv.org/abs/2204.03065},
	abstract = {The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the set of features of a data instance to facilitate downstream matching or grouping related tasks. The transformed set encodes a rich representation of high order relations between the instance features. Distances between transformed features capture their direct original similarity and their third party agreement regarding similarity to other features in the set. A particular min-cost-max-flow fractional matching problem, whose entropy regularized version can be approximated by an optimal transport (OT) optimization, results in our transductive transform which is efficient, differentiable, equivariant, parameterless and probabilistically interpretable. Empirically, the transform is highly effective and flexible in its use, consistently improving networks it is inserted into, in a variety of tasks and training schemes. We demonstrate its merits through the problem of unsupervised clustering and its efficiency and wide applicability for few-shot-classification, with state-of-the-art results, and large-scale person re-identification.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Shalam, Daniel and Korman, Simon},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03065 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Just4Ref, Low, Relative, Unread},
}

@inproceedings{lenc_understanding_2015,
	title = {Understanding image representations by measuring their equivariance and equivalence},
	doi = {10.1109/CVPR.2015.7298701},
	abstract = {Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lenc, Karel and Vedaldi, Andrea},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Computer vision, Convolutional codes, Feature extraction, Histograms, Image reconstruction, Image representation, Just4Ref, Medium, Neural networks, Similarity, Stiching, Unread},
	pages = {991--999},
}

@inproceedings{wang_towards_2018,
	title = {Towards {Understanding} {Learning} {Representations}: {To} {What} {Extent} {Do} {Different} {Neural} {Networks} {Learn} the {Same} {Representation}},
	volume = {31},
	shorttitle = {Towards {Understanding} {Learning} {Representations}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/5fc34ed307aac159a30d81181c99847e-Abstract.html},
	abstract = {It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations.  We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.},
	urldate = {2023-01-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Hu, Zhiqiang and Wu, Yue and He, Kun and Hopcroft, John},
	year = {2018},
	keywords = {High, Just4Ref, Similarity, Unread},
}

@misc{gygli_towards_2020,
	title = {Towards {Reusable} {Network} {Components} by {Learning} {Compatible} {Representations}},
	url = {http://arxiv.org/abs/2004.03898},
	doi = {10.48550/arXiv.2004.03898},
	abstract = {This paper proposes to make a first step towards compatible and hence reusable network components. Rather than training networks for different tasks independently, we adapt the training process to produce network components that are compatible across tasks. In particular, we split a network into two components, a features extractor and a target task head, and propose various approaches to accomplish compatibility between them. We systematically analyse these approaches on the task of image classification on standard datasets. We demonstrate that we can produce components which are directly compatible without any fine-tuning or compromising accuracy on the original tasks. Afterwards, we demonstrate the use of compatible components on three applications: Unsupervised domain adaptation, transferring classifiers across feature extractors with different architectures, and increasing the computational efficiency of transfer learning.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Gygli, Michael and Uijlings, Jasper and Ferrari, Vittorio},
	month = dec,
	year = {2020},
	note = {arXiv:2004.03898 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Medium, Statistics - Machine Learning, Stiching, Unread},
}

@misc{csiszarik_similarity_2021,
	title = {Similarity and {Matching} of {Neural} {Network} {Representations}},
	url = {http://arxiv.org/abs/2110.14633},
	doi = {10.48550/arXiv.2110.14633},
	abstract = {We employ a toolset -- dubbed Dr. Frankenstein -- to analyse the similarity of representations in deep neural networks. With this toolset, we aim to match the activations on given layers of two trained neural networks by joining them with a stitching layer. We demonstrate that the inner representations emerging in deep convolutional neural networks with the same architecture but different initializations can be matched with a surprisingly high degree of accuracy even with a single, affine stitching layer. We choose the stitching layer from several possible classes of linear transformations and investigate their performance and properties. The task of matching representations is closely related to notions of similarity. Using this toolset, we also provide a novel viewpoint on the current line of research regarding similarity indices of neural network representations: the perspective of the performance on a task.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Csiszárik, Adrián and Kőrösi-Szabó, Péter and Matszangosz, Ákos K. and Papp, Gergely and Varga, Dániel},
	month = oct,
	year = {2021},
	note = {arXiv:2110.14633 [cs]},
	keywords = {Computer Science - Machine Learning, High, Similarity, Stiching, Unread},
}

@inproceedings{bansal_revisiting_2021,
	title = {Revisiting {Model} {Stitching} to {Compare} {Neural} {Representations}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/01ded4259d101feb739b06c399e9cd9c-Abstract.html},
	urldate = {2023-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
	year = {2021},
	keywords = {High, Just4Ref, Stiching, Unread},
	pages = {225--236},
}

@inproceedings{barannikov_representation_2022,
	title = {Representation {Topology} {Divergence}: {A} {Method} for {Comparing} {Neural} {Network} {Representations}.},
	shorttitle = {Representation {Topology} {Divergence}},
	url = {https://proceedings.mlr.press/v162/barannikov22a.html},
	abstract = {Comparison of data representations is a complex multi-aspect problem. We propose a method for comparing two data representations. We introduce the Representation Topology Divergence (RTD) score measuring the dissimilarity in multi-scale topology between two point clouds of equal size with a one-to-one correspondence between points. The two data point clouds can lie in different ambient spaces. The RTD score is one of the few topological data analysis based practical methods applicable to real machine learning datasets. Experiments show the agreement of RTD with the intuitive assessment of data representation similarity. The proposed RTD score is sensitive to the data representation’s fine topological structure. We use the RTD score to gain insights on neural networks representations in computer vision and NLP domains for various problems: training dynamics analysis, data distribution shift, transfer learning, ensemble learning, disentanglement assessment.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Barannikov, Serguei and Trofimov, Ilya and Balabin, Nikita and Burnaev, Evgeny},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {Just4Ref, Medium, Similarity, Unread},
	pages = {1607--1626},
}

@misc{li_convergent_2016,
	title = {Convergent {Learning}: {Do} different neural networks learn the same representations?},
	shorttitle = {Convergent {Learning}},
	url = {http://arxiv.org/abs/1511.07543},
	doi = {10.48550/arXiv.1511.07543},
	abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	month = feb,
	year = {2016},
	note = {arXiv:1511.07543 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Just4Ref, Low, Similarity, Unread},
}

@inproceedings{vulic_are_2020,
	address = {Online},
	title = {Are {All} {Good} {Word} {Vector} {Spaces} {Isomorphic}?},
	url = {https://aclanthology.org/2020.emnlp-main.257},
	doi = {10.18653/v1/2020.emnlp-main.257},
	abstract = {Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. “under-training”).},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Vulić, Ivan and Ruder, Sebastian and Søgaard, Anders},
	month = nov,
	year = {2020},
	keywords = {Just4Ref, Low, Similarity, Unread},
	pages = {3178--3192},
}

@inproceedings{soudry_implicit_2022,
	title = {The {Implicit} {Bias} of {Gradient} {Descent} on {Separable} {Data}},
	url = {https://openreview.net/forum?id=r1q7n9gAb},
	abstract = {We show that gradient descent on an unregularized logistic regression problem, for almost all separable datasets, converges to the same direction as the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
	language = {en},
	urldate = {2023-01-19},
	author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Srebro, Nathan},
	month = feb,
	year = {2022},
	keywords = {Just4Ref, Low, Unread},
}

@misc{moschella_relative_2022,
	title = {Relative representations enable zero-shot latent space communication},
	url = {http://arxiv.org/abs/2209.15430},
	doi = {10.48550/arXiv.2209.15430},
	abstract = {Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, distinct latent spaces typically differ by an unknown quasi-isometric transformation: that is, in each space, the distances between the encodings do not change. In this work, we propose to adopt pairwise similarities as an alternative data representation, that can be used to enforce the desired invariance without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, latent isometry invariance, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Moschella, Luca and Maiorca, Valentino and Fumero, Marco and Norelli, Antonio and Locatello, Francesco and Rodolà, Emanuele},
	month = sep,
	year = {2022},
	note = {arXiv:2209.15430 [cs]},
	keywords = {Main, Reading},
}

@misc{kashlak_probability_2022,
	title = {Probability and {Measure}},
	url = {https://sites.ualberta.ca/~kashlak/data/stat571.pdf},
	abstract = {Course notes for STAT 571},
	language = {eng},
	author = {Kashlak, Adam B.},
	year = {2022},
}

@misc{boissonnat_efficient_2018,
	title = {An {Efficient} {Representation} for {Filtrations} of {Simplicial} {Complexes}},
	url = {http://arxiv.org/abs/1607.08449},
	doi = {10.48550/arXiv.1607.08449},
	abstract = {A filtration over a simplicial complex \$K\$ is an ordering of the simplices of \$K\$ such that all prefixes in the ordering are subcomplexes of \$K\$. Filtrations are at the core of Persistent Homology, a major tool in Topological Data Analysis. In order to represent the filtration of a simplicial complex, the entire filtration can be appended to any data structure that explicitly stores all the simplices of the complex such as the Hasse diagram or the recently introduced Simplex Tree [Algorithmica '14]. However, with the popularity of various computational methods that need to handle simplicial complexes, and with the rapidly increasing size of the complexes, the task of finding a compact data structure that can still support efficient queries is of great interest. In this paper, we propose a new data structure called the Critical Simplex Diagram (CSD) which is a variant of the Simplex Array List (SAL) [Algorithmica '17]. Our data structure allows one to store in a compact way the filtration of a simplicial complex, and allows for the efficient implementation of a large range of basic operations. Moreover, we prove that our data structure is essentially optimal with respect to the requisite storage space. Finally, we show that the CSD representation admits fast construction algorithms for Flag complexes and relaxed Delaunay complexes.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Boissonnat, Jean-Daniel and S., Karthik C.},
	month = feb,
	year = {2018},
	note = {arXiv:1607.08449 [cs]},
	keywords = {Just4Ref, Low, Unread},
}

@misc{zhang_convergence_2022,
	title = {On the {Convergence} of {Optimizing} {Persistent}-{Homology}-{Based} {Losses}},
	url = {http://arxiv.org/abs/2206.02946},
	doi = {10.48550/arXiv.2206.02946},
	abstract = {Topological loss based on persistent homology has shown promise in various applications. A topological loss enforces the model to achieve certain desired topological property. Despite its empirical success, less is known about the optimization behavior of the loss. In fact, the topological loss involves combinatorial configurations that may oscillate during optimization. In this paper, we introduce a general purpose regularized topology-aware loss. We propose a novel regularization term and also modify existing topological loss. These contributions lead to a new loss function that not only enforces the model to have desired topological behavior, but also achieves satisfying convergence behavior. Our main theoretical result guarantees that the loss can be optimized efficiently, under mild assumptions.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Zhang, Yikai and Yao, Jiachen and Wang, Yusu and Chen, Chao},
	month = jun,
	year = {2022},
	note = {arXiv:2206.02946 [cs, stat]},
	keywords = {Just4Ref, Medium, Unread},
}

@misc{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	doi = {10.48550/arXiv.1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = feb,
	year = {2017},
	note = {arXiv:1611.03530 [cs]},
	keywords = {Computer Science - Machine Learning, Just4Ref, Low, Unread},
}

@phdthesis{medbouhi_towards_2022,
	title = {Towards topology-aware {Variational} {Auto}-{Encoders} : from {InvMap}-{VAE} to {Witness} {Simplicial} {VAE}},
	shorttitle = {Towards topology-aware {Variational} {Auto}-{Encoders}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-309487},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-18},
	author = {Medbouhi, Aniss Aiman},
	year = {2022},
	keywords = {Medium, Unread},
}

@phdthesis{leitao_topological_2021,
	title = {Topological {Expressiveness} of {Neural} {Networks}},
	url = {https://run.unl.pt/bitstream/10362/129615/1/TAA0115.pdf},
	abstract = {Given a neural network, how many di erent problems can it solve? This important and open
question in deep learning is usually referred to as the problem of the expressive power of a
neural network. Previous research has tackled this issue through statistical and geometrical
methods. This work proposes a new method based on a topological perspective.
Topology is the  eld of mathematics aimed at describing spaces and functions through
robust characterizing features. Topological Data Analysis is the young  eld developed to
extract topological insight from data.
We  rst show that topological features of the decision boundary describe the closest
notion of the intrinsic complexity of a classi cation problem. These topological features divide
classi cation problems into several equivalence classes. Linear-separability is an example of
such a class. We establish the topological expressive power of a network architecture as the
number of di erent topological classes it is able to express.
Being a novel work in a young research  eld, most of the thesis is devoted to developing
this perspective and creating the tools required. The main objective of this thesis is to tackle
neural network’s understanding in general and architecture selection in particular, through a
novel approach.
Our results show that topological expressiveness has a complex correlation with many features in a neural network’s architecture depending weakly on the total number of parameters.
Some of our results recapitulate previous research on geometrical properties, while others are
unique to this novel topological point of view, sometimes challenging previous research.},
	language = {eng},
	author = {Leitão, António},
	year = {2021},
	keywords = {Medium, Unread},
}

@misc{moor_topological_2021,
	title = {Topological {Autoencoders}},
	url = {http://arxiv.org/abs/1906.00722},
	doi = {10.48550/arXiv.1906.00722},
	abstract = {We propose a novel approach for preserving topological structures of the input space in latent representations of autoencoders. Using persistent homology, a technique from topological data analysis, we calculate topological signatures of both the input and latent space to derive a topological loss term. Under weak theoretical assumptions, we construct this loss in a differentiable manner, such that the encoding learns to retain multi-scale connectivity information. We show that our approach is theoretically well-founded and that it exhibits favourable latent representations on a synthetic manifold as well as on real-world image data sets, while preserving low reconstruction errors.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Moor, Michael and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
	month = may,
	year = {2021},
	note = {arXiv:1906.00722 [cs, math, stat]},
	keywords = {High, Unread},
}

@misc{carriere_optimizing_2021,
	title = {Optimizing persistent homology based functions},
	url = {http://arxiv.org/abs/2010.08356},
	doi = {10.48550/arXiv.2010.08356},
	abstract = {Solving optimization tasks based on functions and losses with a topological flavor is a very active, growing field of research in data science and Topological Data Analysis, with applications in non-convex optimization, statistics and machine learning. However, the approaches proposed in the literature are usually anchored to a specific application and/or topological construction, and do not come with theoretical guarantees. To address this issue, we study the differentiability of a general map associated with the most common topological construction, that is, the persistence map. Building on real analytic geometry arguments, we propose a general framework that allows us to define and compute gradients for persistence-based functions in a very simple way. We also provide a simple, explicit and sufficient condition for convergence of stochastic subgradient methods for such functions. This result encompasses all the constructions and applications of topological optimization in the literature. Finally, we provide associated code, that is easy to handle and to mix with other non-topological methods and constraints, as well as some experiments showcasing the versatility of our approach.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Carrière, Mathieu and Chazal, Frédéric and Glisse, Marc and Ike, Yuichi and Kannan, Hariprasad},
	month = feb,
	year = {2021},
	note = {arXiv:2010.08356 [cs, math]},
	keywords = {High, Unread},
}

@inproceedings{poklukar_delaunay_2022,
	title = {Delaunay {Component} {Analysis} for {Evaluation} of {Data} {Representations}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-312715},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-18},
	author = {Poklukar, Petra and Polianskii, Vladislav and Varava, Anastasiia and Pokorny, Florian T. and Kragic, Danica},
	year = {2022},
	keywords = {High, Unread},
}

@article{rieck_neural_2019,
	title = {Neural {Persistence}: {A} {Complexity} {Measure} for {Deep} {Neural} {Networks} {Using} {Algebraic} {Topology}},
	shorttitle = {Neural {Persistence}},
	url = {http://arxiv.org/abs/1812.09764},
	doi = {10.3929/ethz-b-000327207},
	abstract = {While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization. Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.},
	urldate = {2023-01-19},
	author = {Rieck, Bastian and Togninalli, Matteo and Bock, Christian and Moor, Michael and Horn, Max and Gumbsch, Thomas and Borgwardt, Karsten},
	month = feb,
	year = {2019},
	note = {arXiv:1812.09764 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Just4Ref, Low, Mathematics - Algebraic Topology, Statistics - Machine Learning, Unread},
	pages = {25 p.},
}

@misc{chen_topological_2018,
	title = {A {Topological} {Regularizer} for {Classifiers} via {Persistent} {Homology}},
	url = {http://arxiv.org/abs/1806.10714},
	doi = {10.48550/arXiv.1806.10714},
	abstract = {Regularization plays a crucial role in supervised learning. Most existing methods enforce a global regularization in a structure agnostic manner. In this paper, we initiate a new direction and propose to enforce the structural simplicity of the classification boundary by regularizing over its topological complexity. In particular, our measurement of topological complexity incorporates the importance of topological features (e.g., connected components, handles, and so on) in a meaningful manner, and provides a direct control over spurious topological structures. We incorporate the new measurement as a topological penalty in training classifiers. We also pro- pose an efficient algorithm to compute the gradient of such penalty. Our method pro- vides a novel way to topologically simplify the global structure of the model, without having to sacrifice too much of the flexibility of the model. We demonstrate the effectiveness of our new topological regularizer on a range of synthetic and real-world datasets.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Chen, Chao and Ni, Xiuyan and Bai, Qinxun and Wang, Yusu},
	month = oct,
	year = {2018},
	note = {arXiv:1806.10714 [cs, stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Just4Ref, Medium, Statistics - Machine Learning, Unread},
}

@misc{belharbi_neural_2017,
	title = {Neural {Networks} {Regularization} {Through} {Class}-wise {Invariant} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1709.01867},
	doi = {10.48550/arXiv.1709.01867},
	abstract = {Training deep neural networks is known to require a large number of training samples. However, in many applications only few training samples are available. In this work, we tackle the issue of training neural networks for classification task when few training samples are available. We attempt to solve this issue by proposing a new regularization term that constrains the hidden layers of a network to learn class-wise invariant representations. In our regularization framework, learning invariant representations is generalized to the class membership where samples with the same class should have the same representation. Numerical experiments over MNIST and its variants showed that our proposal helps improving the generalization of neural network particularly when trained with few samples. We provide the source code of our framework https://github.com/sbelharbi/learning-class-invariant-features .},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Belharbi, Soufiane and Chatelain, Clément and Hérault, Romain and Adam, Sébastien},
	month = dec,
	year = {2017},
	note = {arXiv:1709.01867 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Just4Ref, Medium, Statistics - Machine Learning, Unread},
}

@misc{verma_manifold_2019,
	title = {Manifold {Mixup}: {Better} {Representations} by {Interpolating} {Hidden} {States}},
	shorttitle = {Manifold {Mixup}},
	url = {http://arxiv.org/abs/1806.05236},
	doi = {10.48550/arXiv.1806.05236},
	abstract = {Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Courville, Aaron and Lopez-Paz, David and Bengio, Yoshua},
	month = may,
	year = {2019},
	note = {arXiv:1806.05236 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Just4Ref, Medium, Statistics - Machine Learning, Unread},
}

@inproceedings{glorot_deep_2011,
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	month = jun,
	year = {2011},
	note = {ISSN: 1938-7228},
	keywords = {Just4Ref, Low, Unread},
	pages = {315--323},
}

@inproceedings{cubuk_autoaugment_2019,
	address = {Long Beach, CA, USA},
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{AutoAugment}},
	url = {https://ieeexplore.ieee.org/document/8953317/},
	doi = {10.1109/CVPR.2019.00020},
	abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classiﬁers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many subpolicies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to ﬁnd the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-theart. Augmentation policies we ﬁnd are transferable between datasets. The policy learned on ImageNet transfers well to achieve signiﬁcant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	month = jun,
	year = {2019},
	keywords = {Just4Ref, Low, Unread},
	pages = {113--123},
}

@misc{hofer_connectivity-optimized_2019,
	title = {Connectivity-{Optimized} {Representation} {Learning} via {Persistent} {Homology}},
	url = {http://arxiv.org/abs/1906.09003},
	doi = {10.48550/arXiv.1906.09003},
	abstract = {We study the problem of learning representations with controllable connectivity properties. This is beneficial in situations when the imposed structure can be leveraged upstream. In particular, we control the connectivity of an autoencoder's latent space via a novel type of loss, operating on information from persistent homology. Under mild conditions, this loss is differentiable and we present a theoretical analysis of the properties induced by the loss. We choose one-class learning as our upstream task and demonstrate that the imposed structure enables informed parameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on computer vision data, these one-class models exhibit competitive performance and, in a low sample size regime, outperform other methods by a large margin. Notably, our results indicate that a single autoencoder, trained on auxiliary (unlabeled) data, yields a mapping into latent space that can be reused across datasets for one-class learning.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Hofer, Christoph and Kwitt, Roland and Dixit, Mandar and Niethammer, Marc},
	month = jun,
	year = {2019},
	note = {arXiv:1906.09003 [cs, math, stat]},
	keywords = {High, Unread},
}
