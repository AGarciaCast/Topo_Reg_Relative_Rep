{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2fde5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from aux_relative_text.multilingual_amazon_anchors import MultilingualAmazonAnchors\n",
    "from typing import *\n",
    "\n",
    "from modules.relAttention import RelativeAttention\n",
    "\n",
    "from datasets import load_dataset, ClassLabel\n",
    "\n",
    "# Tensorboard extension (for visualization purposes later)\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = Path(\"./data\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = Path(\"./saved_models/rel_multi_vanilla\")\n",
    "\n",
    "PROJECT_ROOT = Path(\"./\")\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device=torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d760f6",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f2829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained: bool = True\n",
    "target_key: str = \"class\"\n",
    "data_key: str = \"content\"\n",
    "anchor_dataset_name: str = \"amazon_translated\"  \n",
    "ALL_LANGS = (\"en\", \"es\", \"fr\")\n",
    "num_anchors: int = 768\n",
    "train_perc: float = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fdaa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(lang: str, split: str, perc: float, fine_grained: bool):\n",
    "    pl.seed_everything(42)\n",
    "    assert 0 < perc <= 1\n",
    "    dataset = load_dataset(\"amazon_reviews_multi\", lang)[split]\n",
    "\n",
    "    if not fine_grained:\n",
    "        dataset = dataset.filter(lambda sample: sample[\"stars\"] != 3)\n",
    "\n",
    "    # Select a random subset\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[: int(len(indices) * perc)]\n",
    "    dataset = dataset.select(indices)\n",
    "\n",
    "    def clean_sample(sample):\n",
    "        title: str = sample[\"review_title\"].strip('\"').strip(\".\").strip()\n",
    "        body: str = sample[\"review_body\"].strip('\"').strip(\".\").strip()\n",
    "\n",
    "        if body.lower().startswith(title.lower()):\n",
    "            title = \"\"\n",
    "\n",
    "        if len(title) > 0 and title[-1].isalpha():\n",
    "            title = f\"{title}.\"\n",
    "\n",
    "        sample[\"content\"] = f\"{title} {body}\".lstrip(\".\").strip()\n",
    "        if fine_grained:\n",
    "            sample[target_key] = str(sample[\"stars\"] - 1)\n",
    "        else:\n",
    "            sample[target_key] = sample[\"stars\"] > 3\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(clean_sample)\n",
    "    dataset = dataset.cast_column(\n",
    "        target_key,\n",
    "        ClassLabel(num_classes=5 if fine_grained else 2, names=list(map(str, range(1, 6) if fine_grained else (0, 1)))),\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def _amazon_translated_get_samples(lang: str, sample_idxs):\n",
    "    anchor_dataset = MultilingualAmazonAnchors(split=\"train\", language=lang)\n",
    "    anchors = []\n",
    "    for anchor_idx in sample_idxs:\n",
    "        anchor = anchor_dataset[anchor_idx]\n",
    "        anchor[data_key] = anchor[\"data\"]\n",
    "        anchors.append(anchor)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49e4af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Found cached dataset amazon_reviews_multi (C:/Users/alexg/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4fa8cf12a147fa93572952c541887e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\en\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-3476f9b441626d5a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\en\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-d574878ad23ad156.arrow\n",
      "Global seed set to 42\n",
      "Found cached dataset amazon_reviews_multi (C:/Users/alexg/.cache/huggingface/datasets/amazon_reviews_multi/es/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d361e14b5b4d909182f94a6d812636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\es\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-f863bc7387640b81.arrow\n",
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\es\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-ee0b7780b329a439.arrow\n",
      "Global seed set to 42\n",
      "Found cached dataset amazon_reviews_multi (C:/Users/alexg/.cache/huggingface/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eadd6e9f86e4a39a57b34c0d287c7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\fr\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-e1bd444b80bb0045.arrow\n",
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\fr\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-c79029d9a5b26980.arrow\n",
      "Global seed set to 42\n",
      "Found cached dataset amazon_reviews_multi (C:/Users/alexg/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a2b98253204c2593bf12a316dedacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\en\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-fd9b2139e905e2ea.arrow\n",
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\en\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-73acd18e8f4cf9e6.arrow\n",
      "Global seed set to 42\n",
      "Found cached dataset amazon_reviews_multi (C:/Users/alexg/.cache/huggingface/datasets/amazon_reviews_multi/es/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0863603eebf46009091e89aa636685b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\es\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-5feabbc631f1f845.arrow\n",
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\es\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-efd031a678918e43.arrow\n",
      "Global seed set to 42\n",
      "Found cached dataset amazon_reviews_multi (C:/Users/alexg/.cache/huggingface/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066a5fce52964f668346068bbd0e6944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\fr\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-ee48d5caa215e866.arrow\n",
      "Loading cached processed dataset at C:\\Users\\alexg\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\fr\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-c678b00bf840bf9e.arrow\n"
     ]
    }
   ],
   "source": [
    "train_datasets = {\n",
    "    lang: get_dataset(lang=lang, split=\"train\", perc=train_perc, fine_grained=fine_grained) for lang in ALL_LANGS\n",
    "    }\n",
    "\n",
    "test_datasets = {\n",
    "    lang: get_dataset(lang=lang, split=\"test\", perc=1, fine_grained=fine_grained) for lang in ALL_LANGS\n",
    "    }\n",
    "\n",
    "num_labels = list(train_datasets.values())[0].features[target_key].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a41a58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': Value(dtype='string', id=None),\n",
       " 'product_id': Value(dtype='string', id=None),\n",
       " 'reviewer_id': Value(dtype='string', id=None),\n",
       " 'stars': Value(dtype='int32', id=None),\n",
       " 'review_body': Value(dtype='string', id=None),\n",
       " 'review_title': Value(dtype='string', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'product_category': Value(dtype='string', id=None),\n",
       " 'content': Value(dtype='string', id=None),\n",
       " 'class': ClassLabel(names=['1', '2', '3', '4', '5'], id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(set(frozenset(train_dataset.features.keys()) for train_dataset in train_datasets.values())) == 1\n",
    "class2idx = train_datasets[\"en\"].features[target_key].str2int\n",
    "\n",
    "train_datasets[\"en\"].features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02176880",
   "metadata": {},
   "source": [
    "Get pararel anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3649cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "anchor_dataset2num_samples = 1000\n",
    "anchor_dataset2first_anchors = [\n",
    "        776,\n",
    "        507,\n",
    "        895,\n",
    "        922,\n",
    "        33,\n",
    "        483,\n",
    "        85,\n",
    "        750,\n",
    "        354,\n",
    "        523,\n",
    "        184,\n",
    "        809,\n",
    "        418,\n",
    "        615,\n",
    "        682,\n",
    "        501,\n",
    "        760,\n",
    "        49,\n",
    "        732,\n",
    "        336,\n",
    "    ]\n",
    "\n",
    "\n",
    "assert num_anchors <= anchor_dataset2num_samples\n",
    "\n",
    "pl.seed_everything(42)\n",
    "anchor_idxs = list(range(anchor_dataset2num_samples))\n",
    "random.shuffle(anchor_idxs)\n",
    "anchor_idxs = anchor_idxs[:num_anchors]\n",
    "\n",
    "assert anchor_idxs[:20] == anchor_dataset2first_anchors  # better safe than sorry\n",
    "lang2anchors = {\n",
    "    lang: _amazon_translated_get_samples(lang=lang, sample_idxs=anchor_idxs) for lang in ALL_LANGS\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c21d60a6",
   "metadata": {},
   "source": [
    "This is how we can handdle automatically the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397f6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    encoding = tokenizer(\n",
    "        [sample[data_key] for sample in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "    )\n",
    "    del encoding[\"special_tokens_mask\"]\n",
    "    return  encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c3fc3bd",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da40ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_modules.pl_roberta import LitRelRoberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f4b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2transformer_name = {\n",
    "    \"en\": \"roberta-base\",\n",
    "    \"es\": \"PlanTL-GOB-ES/roberta-base-bne\",\n",
    "    \"fr\": \"ClassCat/roberta-base-french\",\n",
    "    #\"ja\": \"nlp-waseda/roberta-base-japanese\",\n",
    "}\n",
    "assert set(lang2transformer_name.keys()) == set(ALL_LANGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bb70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base\n",
      "PlanTL-GOB-ES/roberta-base-bne\n",
      "ClassCat/roberta-base-french\n"
     ]
    }
   ],
   "source": [
    "train_lang2dataloader = {}\n",
    "test_lang2dataloader = {}\n",
    "anchors_lang2dataloader = {}\n",
    "\n",
    "for lang in ALL_LANGS:\n",
    "    transformer_name = lang2transformer_name[lang]\n",
    "    print(transformer_name)\n",
    "    lang_tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "    train_lang2dataloader[lang] = DataLoader(train_datasets[lang],\n",
    "                                       num_workers=4,\n",
    "                                       collate_fn=partial(collate_fn, tokenizer=lang_tokenizer),\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=32,\n",
    "                                       )\n",
    "    \n",
    "    test_lang2dataloader[lang] = DataLoader(test_datasets[lang],\n",
    "                                       num_workers=4,\n",
    "                                       collate_fn=partial(collate_fn, tokenizer=lang_tokenizer),\n",
    "                                       batch_size=32,\n",
    "                                       )\n",
    "    \n",
    "    anchors_lang2dataloader[lang] = DataLoader(lang2anchors[lang],\n",
    "                                       num_workers=4,\n",
    "                                       collate_fn=partial(collate_fn, tokenizer=lang_tokenizer),\n",
    "                                       shuffle=False,\n",
    "                                       batch_size=32,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c3ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors =  AutoTokenizer.from_pretrained(lang2transformer_name[\"en\"])(\n",
    "        [sample[data_key] for sample in lang2anchors[\"en\"]],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f29e9657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer_model = lang2transformer_name[\"en\"]\n",
    "\n",
    "a = LitRelRoberta(num_labels=num_labels,\n",
    "                 transformer_model=transformer_model,\n",
    "                 anchors=anchors,\n",
    "                 hidden_size=768,\n",
    "                 similarity_mode=\"inner\",\n",
    "                 normalization_mode=\"l2\",\n",
    "                 output_normalization_mode=None,\n",
    "                 dropout_prob=0.1,\n",
    "                 seed=42,\n",
    "                 epochs=5,\n",
    "                 weight_decay=0.01, \n",
    "                 lr_init=1e-3,\n",
    "                 layer_decay=0.9,\n",
    "                 warmup_steps=500,\n",
    "                 device=device\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d7444c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.net.anchors_latent[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "def train_network(lang, seed=24):\n",
    "    \n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    \n",
    "    trainer = pl.Trainer(default_root_dir=CHECKPOINT_PATH / f\"{lang}_seed{seed}\", \n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=EPOCHS, \n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True),\n",
    "                                    LearningRateMonitor(\"step\")])\n",
    "    \n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "    \n",
    "    transformer_model = lang2transformer_name[lang]\n",
    "    anchor_loader = anchors_lang2dataloader[lang]\n",
    "    \n",
    "    model = LitRelRoberta(num_labels=num_labels,\n",
    "                          transformer_model=transformer_model,\n",
    "                          anchor_loader=anchor_loader,\n",
    "                          hidden_size=768,\n",
    "                          similarity_mode=\"inner\",\n",
    "                          normalization_mode=\"l2\",\n",
    "                          output_normalization_mode=None,\n",
    "                          dropout_prob=0.1,\n",
    "                          seed=seed,\n",
    "                          epochs=EPOCHS,\n",
    "                          weight_decay=0.01, \n",
    "                          lr_init=1e-3,\n",
    "                          layer_decay=0.9,\n",
    "                          warmup_steps=500\n",
    "                          )\n",
    "    \n",
    "    train_loader = train_lang2dataloader[lang]\n",
    "    test_loader = test_lang2dataloader[lang]\n",
    "   \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result}\n",
    "    \n",
    "    return model, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_normalize: bool = True ??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = list(range(5))\n",
    "train_classifiers = {\n",
    "    seed: {\n",
    "        embedding_type: {\n",
    "            train_lang: fit(\n",
    "                lang2train_latents[train_lang][embedding_type],\n",
    "                train_dataset[target_key],\n",
    "                seed=seed,\n",
    "                normalize=latent_normalize,\n",
    "            )\n",
    "            for train_lang, train_dataset in tqdm(train_datasets.items(), leave=False, desc=\"lang\")\n",
    "        }\n",
    "        for embedding_type in tqdm([\"absolute\", \"relative\"], leave=False, desc=\"embedding_type\")\n",
    "    }\n",
    "    for seed in tqdm(SEEDS, leave=False, desc=\"seed\")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82e75a59",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca059a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, mean_absolute_error\n",
    "\n",
    "numeric_results = {\n",
    "    \"seed\": [],\n",
    "    \"embed_type\": [],\n",
    "    \"train_lang\": [],\n",
    "    \"test_lang\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"fscore\": [],\n",
    "    \"mae\": [],\n",
    "    \"stitched\": [],\n",
    "}\n",
    "for seed, embed_type2train_lang2classifier in train_classifiers.items():\n",
    "    for embed_type, train_lang2classifier in embed_type2train_lang2classifier.items():\n",
    "        for train_lang, classifier in train_lang2classifier.items():\n",
    "            for test_lang, test_latents in langt2test_latents.items():\n",
    "                test_latents = test_latents[embed_type]\n",
    "                if latent_normalize:\n",
    "                    test_latents = F.normalize(test_latents, p=2, dim=-1)\n",
    "                preds = classifier(test_latents)\n",
    "                test_y = np.array(test_datasets[test_lang][target_key])\n",
    "\n",
    "                precision, recall, fscore, _ = precision_recall_fscore_support(test_y, preds, average=\"weighted\")\n",
    "                mae = mean_absolute_error(y_true=test_y, y_pred=preds)\n",
    "                numeric_results[\"embed_type\"].append(embed_type)\n",
    "                numeric_results[\"train_lang\"].append(train_lang)\n",
    "                numeric_results[\"test_lang\"].append(test_lang)\n",
    "                numeric_results[\"precision\"].append(precision)\n",
    "                numeric_results[\"recall\"].append(recall)\n",
    "                numeric_results[\"fscore\"].append(fscore)\n",
    "                numeric_results[\"stitched\"].append(train_lang != test_lang)\n",
    "                numeric_results[\"mae\"].append(mae)\n",
    "                numeric_results[\"seed\"].append(seed)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "df = pd.DataFrame(numeric_results)\n",
    "df.to_csv(\n",
    "    f\"nlp_multilingual-stitching-amazon-{'fine_grained' if fine_grained else 'coarse_grained'}-{anchor_dataset_name}-{train_perc}.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "df = df.groupby(\n",
    "    [\n",
    "        \"embed_type\",\n",
    "        \"stitched\",\n",
    "        \"train_lang\",\n",
    "        \"test_lang\",\n",
    "    ]\n",
    ").agg([np.mean])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"nlp_multilingual-stitching-amazon-{'fine_grained' if fine_grained else 'coarse_grained'}-{anchor_dataset_name}-{train_perc}.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372200db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# fine_grained: bool = False\n",
    "# anchor_dataset_name: str = \"amazon_translated\" # wikimatrix, amazon_translated\n",
    "# train_perc: float = 0.25\n",
    "\n",
    "# full_df = pd.read_csv(\n",
    "#     f\"nlp_multilingual-stitching-amazon-{'fine_grained' if fine_grained else 'coarse_grained'}-{anchor_dataset_name}-{train_perc}.tsv\",\n",
    "#     sep=\"\\t\",\n",
    "#     index_col=0,\n",
    "# )\n",
    "\n",
    "df = df.groupby(\n",
    "    [\n",
    "        \"embed_type\",\n",
    "        \"stitched\",\n",
    "        \"train_lang\",\n",
    "        \"test_lang\",\n",
    "    ]\n",
    ").agg([np.mean, \"count\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"stitched\", \"seed\", \"precision\", \"recall\"])[full_df.train_lang == \"en\"].groupby(\n",
    "    [\"embed_type\", \"train_lang\", \"test_lang\"]\n",
    ").agg([np.mean, np.std]).round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
