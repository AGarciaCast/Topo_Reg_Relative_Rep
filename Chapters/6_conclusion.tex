\documentclass[../main.tex]{subfiles}
\begin{document}

\chapter{Conclusions and Future work}
\label{ch:conclusionsAndFutureWork}


\section{Conclusions}
\label{sec:conclusions}
In summary, this thesis provides novel insights regarding topological regulation techniques combined with model stitching methods, such as the relative transformation.\\

Firstly, our study led to the development of new theoretical foundations concerning the $\epsilon$-similarities based on the intertwiner groups of activation functions. This advancement allowed for more effective construction of the relative transformation. Additionally, through empirical analysis, we gained a deeper understanding of the similarities within the latent space. As a result, we now have stronger theoretical and empirical support for utilizing the relative transformation in zero-shot model stitching.\\

Secondly, in the full fine-tuning analysis, we observed slightly worse results when replicating the original training setup while incorporating our proposed methodological changes. However, fully fine-tuning the model without freezing the encoder narrowed the performance gap between the new methodology and the results reported in the original paper for the absolute case.\\

Thirdly, our exploration of topological regularization revealed that applying the regularization to the post-relative space is crucial for achieving improved performance. We also found that encouraging overlapping death times distributions resulted in better performance. However, adjusting the post-relative representation proved to be more challenging due to the influence of the images of the anchors.\\

Lastly, the results of model stitching with topological regularization showed improved stitching performance in all stitching combinations and modalities compared to the new baseline, except for one case. Additionally, selecting the same hyperparameters for topological regularization enhanced stitching performance but impacted non-stitching performance.\\

Overall, our findings suggest that the proposed methodological changes and the application of topological regularization can potentially improve the performance of multilingual model stitching. However, further research is needed to explore the new latent space's geometric properties and investigate the use of different metrics for topological regularization.

\section{Limitations}
\label{sec:limitations}

As mentioned previously, the training of the network using the topological densification technique, as originally proposed in \cite{hofer_densified_2021}, was hindered by the limited VRAM of the V100 GPU we utilized. Consequently, we resorted to employing the ``debiasing trick,'' as described earlier, to accommodate this regularization technique. Unfortunately, this computational limitation imposed constraints on our ability to achieve optimal results.

Additionally, the unexpected issue with the topological densification led us to omit certain experiments, such as the analysis of representation similarity in multilingual model stitching.\\

Moreover, due to time constraints, we were unable to conduct an extensive literature study on the possible sources of the observed $\epsilon$-similarity, as initially intended. We believe that conducting further research in this area could potentially unveil additional insights into the origins of the observed isometries.\\

Lastly, our multilingual stitching models employed a single linear layer as the classification head. Although this architecture aligns with the standard approach in certain state-of-the-art transformer models, we aimed to investigate the utility of this regularization technique when using a more expressive classification head. Unfortunately, due to time and computational limitations, we were unable to explore this avenue.

\section{Future work}
\label{sec:futureWork}
There are several potential directions for expanding the work presented in this thesis. Here, we highlight some of the most relevant ones:
\begin{itemize}
    \item \textbf{Investigation of alternative simplicial complex constructions:} As we have seen in Section \ref{sec:witness_topo}, it would be interesting to asses if our results depend on the specific construction to produce simplicial complexes in the TDA-based regularization. One good candidate for a new simplicial complex construction that could be used in the post-relative space is the Lazy Witness complex.

    \item \textbf{Further exploration of metric choices for Vietoris–Rips filtrations:} In Section \ref{sec:res_topo}, we demonstrated the potential benefits of using different metrics to compute the Vietoris–Rips filtrations. However, our analysis provided only a proof of concept. A more comprehensive investigation and analysis of different metrics would be advisable.

    \item \textbf{Analysis of representation similarity in multilingual model stitching:} Due to time constraints, we were unable to analyze the CKA in the multilingual model stitching experiments. Performing this representation similarity analysis would be valuable. To conduct such an analysis, a new dataset would need to be created, similar to the parallel anchor construction, where translations to other languages are available.

    \item \textbf{Testing topological regularization on large models with increased GPU VRAM:} It is expected that better performance could be achieved by using larger VRAM in the GPU, eliminating the need for the ``debiasing fix'' utilized in this work. Therefore, it would be interesting to evaluate the topological densification technique on large models while employing a more powerful GPU.

    \item \textbf{Exploring the combination of topological regularization with the relative transformation in a multimodal setup:} Recent studies have shown that the relative transformation can transform unimodal models into multimodal ones without additional training \cite{norelli_asif_2022}. Investigating how to adapt the proposed methodology to combine the topological densification with the relative transformation in this new multimodal setup would be intriguing.
\end{itemize}


\section{Reflections}
\label{sec:reflections}

As discussed in Section \ref{sec:repLearn}, an application of the cross-domain model stitching procedure is a scenario where multiple devices exist, each operating in a specific language. In such cases, one can achieve satisfactory performance by utilizing a single trained classification head (e.g., trained on the English corpus) and stitching it with language-specific encoders. This computational approach proves advantageous in situations requiring frequent model updates, as it necessitates updating only one model instead of all.\\

The observation that specific regularization techniques, such as the topological densification, can enhance model stitching performance further supports their utilization. By employing these improved model stitching setups, we can mitigate computational costs associated with training and fine-tuning networks. It is important to note that this approach would yield not only economic benefits but also have positive environmental implications by reducing the immense carbon footprint associated with training large language models \cite{strubell_energy_2019}.

\end{document}